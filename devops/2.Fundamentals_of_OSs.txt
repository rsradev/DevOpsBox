Module 2 - Fundamental of Operating Systems

P
What is an Operating System (OS)
Software layer
Manages computer hardware
Provides services for computer programs
Acts as an intermediary between users and the hardware
An operating system, or OS, is essentially the backbone of any computer system. It’s a layer of software that sits between the user and the computer’s hardware.

Firstly, the OS manages the computer’s hardware—things like the CPU, memory, storage, and input/output devices like keyboards, mice, and printers. Without an operating system, users would have to manually control each piece of hardware, which would be extremely complex.

Next, the OS provides services to computer programs. When we run an application—whether it’s a browser, a game, or even a simple text editor—the operating system manages how that program uses the system’s resources. It allocates memory, keeps track of processes, and handles data transfers.

One of the most important roles of an operating system is that it acts as an intermediary between the user and the hardware. Without an OS, we would have to interact directly with the hardware, which would involve complicated instructions just to perform basic tasks. Instead, the operating system translates user commands into instructions the hardware can understand and processes the output for us in a more human-friendly way.

In short, an operating system creates a user-friendly environment where we can run programs and use the computer’s hardware without needing to understand the technical details.å

P
Operating system main functions
Resource Management
Process and Thread Management
Process Isolation
Memory Management
Input/Output Control
File System Management
Security and Permissions
In addition to providing a platform for running programs, the operating system has the critical task of managing both hardware and software resources.

Starting with resource management, the OS controls essential hardware components like the CPU, memory, disk drives, and peripheral devices. It ensures that all resources are used efficiently and that hardware components are coordinated to function properly.

Next is process and thread management. The OS allocates CPU time to different running applications. Since the CPU can only perform one task at a time, the OS manages multiple tasks by switching between them rapidly, giving the appearance that everything is happening simultaneously. This process scheduling is crucial to keeping your system responsive.

Another important function is process isolation. The OS makes sure that applications are isolated from one another so they don’t interfere with each other’s operation. If one application crashes, it won’t take down the whole system, ensuring that the others continue to function.

In terms of memory management, the operating system is responsible for allocating and deallocating memory as needed by applications. When you run a program, it needs memory to function, and the OS ensures it gets the required space while preventing applications from stepping on each other’s memory, which ensures system stability.

Input/Output control is another critical function. The OS handles communication between the system and external devices like printers, USB drives, and keyboards. It standardizes how hardware communicates, so programs don’t have to worry about hardware specifics.

When it comes to file system management, the OS organizes how data is stored and retrieved on devices like hard drives. It maintains directories, keeps track of where files are stored, and ensures you can access your data quickly and efficiently.

Lastly, security and permissions are fundamental to any modern operating system. The OS provides mechanisms for protecting data, authenticating users, and controlling who can access files, folders, and system resources. This is essential for preventing unauthorized access and maintaining data privacy.

P
User vs Developer
OS Developer point of view

Provides application programming interfaces
Provides common services
OS User point of view

Operating system is there to execute programs
Provides easy interface
When we think about operating systems, it’s important to recognize that the perspectives of users and developers are quite different. Let’s break down each point of view.

From the OS Developer point of view, the operating system is much more than just a tool to run programs. Developers see the OS as a platform that provides application programming interfaces (APIs). These APIs allow developers to write software that can interact with the system’s hardware and services without having to manage every low-level detail. For example, if a developer needs to write an application that saves files to disk, the OS provides APIs that handle the complexity of reading, writing, and organizing files.

Additionally, developers rely on the OS to provide common services that support the smooth operation of their programs. These services include things like memory management, file systems, security, and process management. Without the OS handling these tasks, developers would have to write additional code to manage resources directly, which would make software development far more difficult and time-consuming.

On the other hand, from a User point of view, the OS has a much simpler role. For users, the operating system is simply there to execute programs. Users don’t need to understand the technical complexities that developers deal with. They just want to run their applications—whether it’s a web browser, a video editor, or a game—and have the system manage the rest. Users expect the OS to handle tasks in the background, like managing memory and CPU resources, without interrupting their experience.

Moreover, the OS provides an easy interface for users to interact with the system. Whether it’s a graphical user interface (GUI) or even a command-line interface (CLI), users want simplicity. The interface allows them to manage files, install software, or customize settings in an intuitive way. This ease of use is critical for a good user experience—users shouldn’t have to think about the underlying complexities, and the OS should make the process of running and using applications as smooth as possible.

P
Hardware abstraction
OS provides hardware abstraction for applications

Applications don’t need to know specific hardware details

Can run on different processors (e.g., Intel, AMD)
No need for separate versions for different processors
Works with different storage types and I/O devices transperantly (e.g. HDD, SSD)
Unaware of how devices are connected (USB, network, wireless)
No need to know network connection type (Wi-Fi, Ethernet)
Multiple applications can share the same hardware simultaneously.

When we talk about hardware abstraction, we mean that the operating system acts as a middle layer between applications and the hardware. The OS provides a standard interface, allowing applications to work without needing to understand or directly interact with the details of the underlying hardware.

For example, applications don’t need to know the specific type of processor they’re running on—whether it’s from Intel or AMD. As long as the processors share the same architecture, the OS handles any differences, so developers don’t have to create separate versions of their software for each processor type.

Similarly, applications can work with different storage types—whether it’s a traditional hard drive (HDD) or a solid-state drive (SSD)—without needing to adjust their behavior. The operating system manages the differences in how these devices read and write data, allowing the application to focus on its main tasks.

Another key benefit is that applications are unaware of how devices are connected to the system. Whether a peripheral device, like a printer or keyboard, is connected via USB, over a network, or wirelessly, the OS provides a unified way to interact with those devices. This saves developers from writing code for every possible connection type.

Applications also don’t need to concern themselves with how the computer connects to the network. Whether the connection is through Wi-Fi, Ethernet, or any other method, the operating system abstracts those details, ensuring the application can communicate without worrying about the specifics of the network.

Lastly, the OS enables multiple applications to share the same hardware simultaneously. This means that while one application uses the CPU, memory, and storage, another can run alongside it, all managed seamlessly by the operating system. This is critical for multitasking and efficient use of system resources.

P
History and Evolution of Operating Systems
Early Systems (1940s-1950s)
Mainframe Era (1960s)
The Rise of Unix (1970s)
Personal Computers Era (1980s)
Modern Operating Systems (1990s-Present)
Early Systems (1940s-1950s) The earliest computers didn’t have operating systems. They were large, single-purpose machines that performed one task at a time. Users would load programs manually using punch cards, and the system would execute them sequentially in what’s known as batch processing. During this era, there was no concept of multi-tasking, and each program had direct control over the hardware. Early users were engineers and mathematicians who had to interact closely with the hardware, often rewiring parts of the machine to perform specific tasks.

Mainframe Era (1960s) By the 1960s, as computers became more sophisticated and began to be used for business and scientific purposes, early operating systems started to emerge to handle the growing complexity. IBM introduced OS/360 in 1964, a revolutionary system that could run on a family of compatible hardware, making it one of the first true operating systems as we know them. Around the same time, time-sharing systems like CTSS and Multics were developed. These systems allowed multiple users to work on a computer simultaneously by sharing CPU time, a concept that was a precursor to the modern multiuser environments.

The Rise of Unix (1970s) A significant milestone in OS history was the creation of UNIX in 1969 by AT&T’s Bell Labs. Unix was designed with portability and flexibility in mind, allowing it to run on various hardware platforms. It introduced several core concepts like multitasking, hierarchical file systems, and multiuser capabilities that became standard in later operating systems. Unix became the foundation for many other systems and had a lasting influence on the development of both modern server systems and consumer operating systems.

Personal Computers (1980s) The 1980s saw the rise of personal computing, making operating systems more accessible to the general public. Microsoft’s MS-DOS (1981) became the dominant OS for IBM-compatible personal computers, using a command-line interface that required users to type text commands. Apple’s macOS (1984) changed the game by introducing a graphical user interface (GUI), allowing users to interact with the computer using visual icons and a mouse, making it far easier to use. Microsoft followed suit with Windows in 1985, initially as a graphical layer over MS-DOS, which evolved over time into a fully independent operating system.

Modern Operating Systems (1990s-Present) The 1990s marked the beginning of truly modern operating systems. Windows NT, introduced by Microsoft in 1993, brought a new architecture designed for enterprise use, with improved security and stability. This line eventually became the basis for all future versions of Windows, from XP to Windows 11. Around the same time, Linux, an open-source operating system based on Unix principles, gained popularity. Its flexibility and open nature made it the OS of choice for servers, supercomputers, and embedded systems. In the 2000s, the rise of smartphones ushered in the mobile operating system revolution. Apple’s iOS (2007) and Google’s Android (2008) brought the power of modern operating systems to mobile devices, blending the functionality of traditional OSs with touch-based interfaces. Today, these mobile OSs dominate the computing landscape, with billions of devices running on these platforms globally.

P
Operating system types
Desktop

Windows
Linux
MacOS
More…
Server

Linux
Windows
Unix – BSD, AIX, HPUX, etc.
More…
Mobile

Android
IOS
More…
Embedded

Linux
Minix
Windows CE
More…
Real-time (RTOS)

QNX
RTLinux
Windows CE
More…
Operating systems come in various forms, each designed to meet different needs depending on the device and the user requirements. Let’s explore the main categories of operating systems:

First, we have Desktop Operating Systems. These are the ones we interact with most often in our everyday computing tasks.

Windows is the most widely used desktop OS, known for its user-friendly interface and extensive software compatibility.
Linux is a popular choice for those who prefer open-source systems and more control over their OS environment.
MacOS is the default OS for Apple computers, offering a seamless experience within the Apple ecosystem.
There are many others as well, but these are the most common.
Next, we have Server Operating Systems. These are designed to handle large-scale tasks like hosting websites, managing databases, and running enterprise applications.

Linux is again very popular in server environments due to its stability, security, and flexibility.
Windows Server is commonly used for businesses that rely on Microsoft’s ecosystem.
Unix-based systems, like BSD, AIX, and HPUX, are also strong players in this space, known for their reliability and performance, especially in large-scale enterprise environments.
Moving on to Mobile Operating Systems, which power our smartphones and tablets.

Android, developed by Google, dominates the mobile market with its open-source nature and wide device compatibility.
iOS, by Apple, is the OS for iPhones and iPads, known for its optimized performance and security.
There are a few other mobile OSs, but Android and iOS are by far the most prominent today.
We also have Embedded Operating Systems. These are used in specialized devices such as IoT devices, routers, and industrial machines.

Linux is commonly used in embedded systems due to its flexibility and small footprint.
Minix is another lightweight OS used for educational and embedded applications.
Windows CE is a version of Windows designed specifically for embedded devices, offering real-time capabilities for more demanding embedded tasks.
Finally, there are Real-time Operating Systems (RTOS). These are designed for applications that require precise timing and reliability, often found in industries like automotive, aerospace, and telecommunications.

QNX is a widely used RTOS for mission-critical systems.
RTLinux is a version of Linux designed to meet real-time requirements.
Windows CE can also be configured for real-time operations, making it suitable for certain embedded and industrial applications.
Each type of OS is tailored to specific use cases, balancing user needs with the hardware they run on, from personal computing to real-time industrial applications.

Operating System Architectures



Implements core libraries and mechanisums to allow applications to access the hardware resources
Applications can make requests to the operating system via syscalls (system calls)
Can alert applications via singnals
In this slide, we’re looking at a simplified view of an operating system’s architecture, which is divided into two key spaces: User Space and Kernel Space. Let’s break it down.

Starting with User Space, this is where user applications run. These are the programs we interact with daily, like browsers, word processors, or games. The operating system provides these applications with the services they need through core libraries. These libraries contain essential functions for interacting with the system, such as opening files, allocating memory, and communicating with hardware.

The operating system offers these services through system calls, or syscalls. When an application needs to perform a task that requires access to hardware—like writing to a file—it doesn’t do this directly. Instead, it makes a system call, asking the OS to handle the task on its behalf. This is where Kernel Space comes into play.

Kernel Space is where the OS kernel resides, and this is the heart of the operating system. The kernel is responsible for managing the system’s hardware resources and making sure they are allocated efficiently. It includes several critical components like the Memory Manager, which handles memory allocation; the Process Scheduler, which determines which processes get CPU time; and the I/O Manager, which manages input and output devices.

The Kernel Modules/Drivers are part of Kernel Space as well. These are responsible for controlling specific hardware devices like storage, USB devices, and network devices. The kernel interacts with these drivers to ensure that the hardware functions properly and that the operating system can communicate with these devices seamlessly.

In this architecture, the kernel can also interact with applications by sending signals. These are alerts that inform applications of specific events, like when a process has been interrupted or when a system resource has become available.

The kernel provides a layer of hardware abstraction, meaning applications don’t need to worry about the specifics of the hardware they’re running on. Whether it’s managing memory, the CPU, or input/output devices, the kernel takes care of it all, allowing applications to focus on their tasks without needing to understand the details of the underlying hardware.

In summary, this architecture ensures that applications can access hardware resources in a secure, controlled manner through system calls, and that the operating system can manage hardware resources efficiently, isolating applications from hardware complexities.

P
Kernel
What is a Kernel?

The core component of the operating system
Acts as a bridge between software applications and the hardware of a computer
Provides essential services for applications
Provides abstraction of hardware components
The kernel is the heart of any operating system and plays a vital role in ensuring everything functions smoothly. Let’s break down what the kernel is and why it’s so important.

First, what is a kernel? It’s the core component of the operating system, responsible for managing communication between software applications and the computer’s hardware. Every time an application needs to access hardware resources—whether it’s to save a file, use the CPU, or send data over the network—the request goes through the kernel.

The kernel acts as a bridge between software and hardware, ensuring that applications can interact with hardware components like the CPU, memory, and input/output devices without needing to know the intricate details of how the hardware operates. This separation is crucial because it simplifies application development and ensures that software can run on different hardware systems without modification.

Next, the kernel provides essential services for applications. This includes memory management, process scheduling, input/output control, and file system management. Essentially, the kernel manages system resources efficiently, ensuring that multiple applications can run simultaneously without interfering with each other.

Another key feature of the kernel is that it provides hardware abstraction. This means that applications don’t need to directly control the hardware components. Instead, the kernel abstracts, or hides, the complexity of the underlying hardware, offering a simplified interface for software to interact with. Whether you’re using different types of processors, storage devices, or peripherals, the kernel ensures that applications don’t need to be aware of these differences. The kernel handles these tasks internally, allowing developers to focus on building their applications rather than managing hardware differences.

In summary, the kernel is the engine that makes the operating system work. It manages the core functions of the system, bridges the gap between software and hardware, and ensures that everything runs efficiently and securely.

P
Monolithic vs. Microkernel
Monolithic Kernel

All core functions (process, memory, device, and file management) run in a single space for efficiency
Microkernel

Minimizes kernel functions
Delegates tasks to drivers
Hybrid Kernel

Combines aspects of both monolithic and microkernels
Aims to balance performance and modularity
In the world of operating systems, there are different ways the kernel can be structured. The two main types are the Monolithic Kernel and the Microkernel, and there’s also a combination of the two called the Hybrid Kernel. Let’s explore each one.

First, let’s talk about the Monolithic Kernel. In this architecture, all the core functions of the operating system—such as process management, memory management, device management, and file system management—run in a single space known as kernel space. The key advantage here is efficiency. Since all the core components are tightly integrated and run together in one space, the system can execute tasks quickly and efficiently. However, the downside is that if there’s a bug or issue in one part of the kernel, it can affect the entire system. That’s why monolithic kernels are considered more powerful but potentially less stable than other designs.

Next, we have the Microkernel architecture. This design takes a different approach by minimizing the functions that run in the kernel. Instead of handling all core services, the microkernel only manages the most basic tasks, like process scheduling and inter-process communication. Everything else—like device drivers, file systems, and network management—is pushed out and handled by separate modules or drivers. The advantage of this design is that it’s more modular and stable. If something goes wrong in a driver or a module, it won’t crash the entire system. However, this design can be less efficient because there’s more overhead involved in communicating between the kernel and the user space modules.

Also, we have the Hybrid Kernel, which is a combination of both the monolithic and microkernel approaches. It aims to balance performance and modularity. In a hybrid kernel, most core functions still run in kernel space like in a monolithic kernel, but some tasks, particularly device drivers, are moved to user space as in a microkernel. This structure tries to get the best of both worlds: the performance of a monolithic kernel and the modularity and stability of a microkernel. A good example of a hybrid kernel is the one used by Windows and macOS, where certain functions are modular while others remain tightly integrated for speed.

Finally, Linux fundamentally uses a monolithic design - All core operating system services—such as process management, memory management, device drivers, and file systems—run in kernel space, making it highly efficient for performance. However, Linux supports loadable kernel modules (LKMs), which allow certain functionalities, like device drivers or file systems, to be added or removed dynamically without rebooting the system. This modularity gives it some flexibility, but at its core, Linux remains a monolithic kernel because all of its critical services run within the kernel itself. In Windows and MacOS some of those critical services are splited between user space and kernel space and for this reason they are considered Hybrid by desing.

In summary, the key difference between these kernel types lies in how they handle core OS functions. The monolithic kernel focuses on performance by keeping everything together, the microkernel prioritizes stability and modularity by minimizing the kernel’s responsibilities, and the hybrid kernel tries to strike a balance between the two.”

P
Partitioning
What is Partitioning?

Dividing a physical disk into separate logical sections
Why Partition?

Organize data and separate different uses (e.g., OS, data storage, temporary storage)
Improve data management and performance
Can be formated with different file systems
Next we talk about partitioning, an important concept in disk management.

So, what is partitioning? Partitioning is the process of dividing a physical disk into separate logical sections, known as partitions. Think of it as splitting a single hard drive into multiple smaller 'virtual' drives, each of which can be managed independently. For example, you could have one partition for your operating system, another for your personal files, and yet another for backups.

Now, why would we want to partition a disk? There are several key reasons:

Organizing data: Partitioning helps you keep things organized. For instance, you can create separate partitions for different purposes—one for the operating system, another for data storage, and perhaps one for temporary files. This makes it easier to manage your data, and in case of system issues, your personal files remain safe if they are stored on a different partition.
Improving performance: Partitioning can help boost system performance. By separating system files from user data, you reduce fragmentation, which can lead to faster file access times. Also, the operating system can perform better when it’s isolated on its own partition.
Flexibility with file systems: Another advantage of partitioning is that each partition can be formatted with a different file system. For example, you could format one partition with EXT4 for a Linux operating system, and another with NTFS for data that needs to be accessible on both Linux and Windows systems. This allows you to tailor each partition to the specific needs of the data or applications stored there.
In addition to partitioning physical disks, it’s important to note that you can also create partitions on other types of storage:

Logical Volumes: Using tools like Logical Volume Manager (LVM), you can create partitions that span multiple physical disks, giving you more flexibility to resize and manage partitions.
Network File Systems: You can partition storage that’s hosted on a network, allowing multiple systems to access different sections of the same networked storage.
RAID Arrays: Partitions can also be created on RAID (Redundant Array of Independent Disks) setups, where multiple disks work together to improve performance or data redundancy.
In summary, partitioning is a key tool in disk management. It allows us to organize data, improve performance, and provide flexibility by using different file systems for different purposes—not just on physical disks, but also on logical volumes, network storage, and RAID arrays.

P
File Systems
What is a File System?

Defines how data is stored and retrieved on a device
Functions of a File System:

Organizes and manage files
Enables access control and permissions
Supports metadata for files (timestamps, size, etc.)
Now, let’s discuss file systems, a critical component of any operating system.

What is a file system? At its core, a file system defines how data is stored and retrieved on a device, whether it’s a hard drive, solid-state drive, or any other storage medium. It essentially provides the structure that allows your operating system to keep track of where files are located and how they can be accessed.

File systems have several important functions:

Organizes and manages files: The primary role of a file system is to organize data in a way that makes it easy to locate and manage. Files are stored in directories or folders, and the file system ensures that the correct data is retrieved when you access a file. Without a file system, we wouldn’t have a structured way to store and access data on storage devices.

Enables access control and permissions: File systems also manage who can access or modify files. For example, you might have files that only you can open, while other files can be shared across different users. The file system enforces these rules by controlling access permissions for reading, writing, or executing files. This is especially important in multi-user environments, where different people need different levels of access.

Supports metadata for files: File systems also track metadata—the extra information about a file, such as its creation date, size, and the last time it was modified. This metadata is stored alongside the file itself and can be used by the operating system and applications to display file properties, sort files, or check when a file was last accessed.

In summary, a file system is essential for storing, organizing, and retrieving data efficiently, and it provides additional functions like access control and metadata support to ensure files are managed securely and correctly.

P
Files and Directories
What is a file?

A file is a logical grouping of related data
It is identified by a filename
What is a directory?

A directory is a hierarchical collection of files and other directories.​
Let’s dive into two fundamental concepts in any operating system: files and directories.
First, what is a file? A file is essentially a logical grouping of related data. It can contain anything from a text document, an image, or even an executable program. Think of a file as a container for storing information that the operating system can read and interpret. Every file is identified by a unique filename, which allows both the operating system and the user to locate and manage the file. For example, if you save a document as ‘report.docx,’ that filename is how you or your system will refer to it when you want to access, modify, or delete it.

Now, what is a directory? A directory is a hierarchical collection of files and other directories. You can think of it like a folder that organizes your files. In fact, on many systems, 'folder' and 'directory' are used interchangeably. Directories allow us to create a structured organization for our files, making it easier to manage and locate them. For example, you might have a ‘Documents’ directory that contains files like reports, spreadsheets, and presentations, and within that, you might have subdirectories like ‘Work’ or ‘Personal’ for further organization.

One of the key things about directories is that they can contain not just files, but also other directories, creating a hierarchical structure. This structure allows you to logically organize your data and make navigation more intuitive. For instance, on most systems, your files are stored within a 'home' directory, which might have subdirectories like 'Documents', 'Pictures', and 'Downloads.'

In summary:

A file is a logical unit of data, identified by a unique filename.
A directory is a hierarchical collection of files and possibly other directories, providing structure and organization for data management.
This basic file-and-directory structure is essential for keeping large volumes of data organized and easy to navigate.

P
Processes
What is a process?

Represents an instance of a running program
OS create a process to run a program
Starting an application creates a process
Now let’s talk about processes, another fundamental concept in operating systems.

So, what is a process? In simple terms, a process represents an instance of a running program. Every time you start a program—whether it’s a web browser, a game, or a text editor—the operating system creates a process to manage and execute that program.

Here’s how it works: When you start a program, the operating system doesn’t just run the application directly. Instead, it creates a process that acts as a container for the program to run. This process holds everything the program needs to function—such as memory allocation, CPU time, and access to files or other resources. The process is what allows the program to be executed and controlled by the operating system.

To put it another way, starting an application creates a process. For example, if you launch a browser, the operating system starts a process that manages the browser’s code, handles its interactions with the hardware, and tracks its usage of system resources. Multiple processes can exist simultaneously—one for each running program or task. This is how modern operating systems allow multitasking, where multiple programs can run side-by-side without interfering with each other.

The operating system is responsible for managing these processes: it allocates resources, schedules CPU time, and ensures that processes don’t conflict with one another. This management is key to making sure your system runs smoothly, even when multiple applications are running at the same time.

In summary:

A process is an instance of a program that is actively running.
The operating system creates and manages processes whenever you start a program.
Starting an application, like a browser or a game, creates a process that the OS controls and monitors to ensure smooth execution.

Inter Process Communication (IPC)
What is IPC?

Mechanism that allows processes to exchange data and communicate
Enables coordination between different processes to perform complex tasks and share information efficiently
Why is IPC Important?

Resource Sharing
Process Synchronization
Modularity and Efficiency
Let’s begin by understanding what Inter-Process Communication (IPC) is and why it’s essential for modern computing systems.
What is IPC?
Inter-Process Communication (IPC) is a mechanism that allows different processes running on the same system, or even across different systems, to exchange data and communicate with each other. For instance, in an operating system, there could be multiple processes running concurrently, and IPC enables these processes to coordinate and share information to complete more complex tasks. For example, one process might produce data that another process consumes, so IPC provides the means for this communication to occur in an organized and efficient way. Why is IPC Important?

Resource Sharing
IPC enables processes to share resources like memory, files, and input/output devices. Without IPC, each process would need its own resources, leading to inefficiency and duplication. For example, in an operating system, IPC allows multiple processes to share access to the same file or memory location.

Process Synchronization
When multiple processes execute concurrently, there is a need to synchronize their execution to avoid conflicts. For example, two processes trying to write to the same file at the same time can result in data corruption. IPC provides synchronization mechanisms like semaphores and mutexes to control access to shared resources.

Modularity and Efficiency
IPC allows software to be more modular. You can design large applications by splitting them into smaller processes that communicate through IPC. This modular approach makes the system easier to manage, debug, and maintain. Additionally, IPC helps to improve the efficiency of communication between processes by ensuring that the communication happens smoothly and without duplication of resources. In summary, IPC is fundamental to managing how processes in a system interact and work together, ensuring resource sharing, synchronization, and efficient process management.

P
Common IPC Mechanisms
Pipes
Message Queues
Shared Memory
Semaphores/Mutex
Sockets
Unix Sockets
Now let’s look at some common Inter-Process Communication (IPC) mechanisms. These mechanisms allow processes to exchange data or signals, depending on the type of communication needed.

Pipes
Pipes provide a one-way communication channel between two processes. Data written by one process into the pipe can be read by another process. It’s commonly used in Unix/Linux systems for simple data transfers between processes.

Message Queues
Message Queues allow processes to send and receive messages asynchronously. The message is stored in a queue until the receiving process retrieves it. This is useful when processes need to communicate independently of each other’s timing.

Shared Memory
In Shared Memory, two or more processes are allowed to access the same memory space. This is the fastest IPC method because it avoids the need for data to be copied between processes. However, managing shared memory requires proper synchronization to prevent conflicts.

Semaphores/Mutex
Semaphores and mutexes are used for synchronization. They control access to shared resources by ensuring that only one process can access a critical section of the code at a time, preventing race conditions. Semaphores can manage multiple instances of a resource, while mutexes are typically used to manage a single resource at a time.

Sockets Sockets allow for communication between processes over a network. They are used in client-server models, where a server listens for connections from clients and exchanges data over the network. Sockets are fundamental for processes communicating over local networks or the internet. In summary, these IPC mechanisms allow processes to communicate and coordinate, depending on the requirements of the task at hand. Each method has its specific use case, and choosing the right one depends on factors like speed, synchronization, and the complexity of the data exchange.

Unix Sockets
Unix Sockets are a form of Inter-Process Communication (IPC) that allows processes to communicate with each other through a file descriptor. Unlike network sockets, which are used for communication over a network, Unix sockets are designed specifically for communication within the same machine.

P
What is a Daemon or Service?
A Daemon is a long running process that operates in background

Provides specific function that is designed to require no user intervention
Can be configured to start when the operating system is started
Also known as Service in Windows
Example of daemons:

Web Server (Apache, Nginx, IIS)
Database Server (MySql, MongoDB, MSSQL)
More…
Now let's discuss daemons, a special type of processes that play an essential role in operating systems.
A daemon is a long-running process that operates quietly in the background, providing critical services without any direct user interaction. Unlike regular processes, which are started when a user launches an application, daemons are usually designed to run continuously, often starting up automatically when the operating system boots.

For example, many daemons perform system-level tasks like managing network connections, handling web traffic, or managing databases. Because these processes don’t require ongoing input from the user, they can perform their functions in the background while users focus on other tasks.

One key point is that daemons provide specific functions that require no user intervention. Once they’re running, they perform their job automatically. For instance, a web server daemon, like Apache or Nginx, handles incoming web requests and serves web pages to users. The daemon does this without the need for anyone to monitor or control it directly—it just runs in the background, ensuring the server is available and responsive.

Daemons are often configured to start when the operating system is started. This ensures that essential services are ready as soon as the system is up and running. For instance, a database server like MySQL or MongoDB might start as a daemon immediately when the system boots, so that any application needing database access can connect right away.

In Windows, the term for daemons is Services. The concept is essentially the same—services are background processes that handle system tasks and provide essential services without user interaction. You can also see often the term service in Linux documents.

Here are some common examples of daemons:

Web Servers like Apache, Nginx, or IIS are daemons that handle web traffic and serve content to users.
Database Servers like MySQL, MongoDB, or MSSQL are daemons that manage databases, allowing other applications to read, write, and manipulate data.
In summary, daemons are background processes that handle specific tasks without user intervention. They are crucial for running many system-level services and are often configured to start with the OS, ensuring that the system remains functional and responsive.”

P
Multitasking
CPU Core == Single Task

Only one process can be executing at any one time on a single CPU core
What is Multitasking?

Allow multiple processes to share same CPU and other system resources
Multitasking operating system

OS switches between processes to give the appearance of many processes executing simultaneously
Let’s start by talking about CPU cores and how they handle tasks.

A CPU core can execute only one process at a time. So, on a single core, only one task can be actively running at any given moment. This means that if you have a system with a single CPU core, only one process is fully utilizing the core at any given time. While it may seem like multiple programs are running simultaneously, what’s actually happening is that each core can focus on only one task or process at a time.

So, what about when you’re running multiple programs at once? This brings us to multitasking.

What is multitasking? Multitasking is the ability of an operating system to allow multiple processes to share the same CPU and other system resources. Since a single CPU core can handle only one process at a time, multitasking gives the illusion that several processes are running simultaneously. The operating system manages this by switching rapidly between tasks, giving each process a small slice of the CPU’s time.

Now, in a multitasking operating system, the OS switches between processes quickly to make it seem like all of them are running at once. This switching is known as context switching, where the OS saves the current state of one process and loads the state of the next process. This happens so fast that users don’t notice the switch, which gives the appearance of many processes executing simultaneously. For example, you might be running a web browser, a music player, and a text editor at the same time. The operating system is switching between these processes so efficiently that it feels like everything is running together smoothly.

To sum up:

A single CPU core can handle only one process at a time.
Multitasking allows multiple processes to share the same CPU and system resources.
In a multitasking operating system, the OS switches rapidly between processes, creating the illusion that all processes are running simultaneously.”

P
Cooperative vs Preemptive Multitasking
Cooperative multitasking

Processes decide for how long it keeps the CPU
Preemptive multitasking

Processes are not in control for how long they are going to use the CPU
When we talk about multitasking in operating systems, there are two main types: Cooperative Multitasking and Preemptive Multitasking. Let’s break down how each one works.

First, in Cooperative Multitasking, the processes themselves decide for how long they keep the CPU. This means that each process voluntarily gives up control of the CPU when it’s done with its task or needs to wait for something (like input/output operations). In this model, the operating system doesn’t force processes to stop; instead, the process must ‘cooperate’ by yielding control back to the OS when it no longer needs the CPU.

While this approach sounds fair, it comes with a major drawback. If a process doesn’t yield the CPU—whether due to a programming error or simply because it’s designed to use the CPU for long periods—other processes must wait their turn, potentially leading to system slowdowns or freezes. Cooperative multitasking relies heavily on the good behavior of each process, which can lead to inefficiencies.

Next, we have Preemptive Multitasking, which solves this issue by placing the operating system in control. In preemptive multitasking, processes do not control how long they will use the CPU. Instead, the OS manages this by assigning a fixed time slice or time period for each process. When the time is up, the OS preempts or interrupts the process and switches to another, ensuring that no single process can dominate the CPU for too long. This gives the OS much finer control over CPU scheduling, resulting in a more responsive system, even when many processes are running. Most modern operating systems, like Windows, macOS, and Linux, use preemptive multitasking because it ensures better system stability and fairness among processes.

To summarize:

In Cooperative Multitasking, the processes themselves decide when to release the CPU, which can cause delays if a process holds the CPU for too long.
In Preemptive Multitasking, the operating system is in charge, ensuring that processes are regularly switched so no single process can dominate the CPU.
Preemptive multitasking is generally the preferred method in modern systems, as it ensures smoother multitasking and a more responsive user experience.

P
Protection Rings (a.k.a. Isolation Rings)
Protects data and functionality from faults
Hardware-enforced by CPU
CPU microcode

Next, let’s talk about Protection Rings, also known as Isolation Rings. These are an important security feature built into modern operating systems and hardware, designed to protect data and functionality from faults or malicious behavior.

In simple terms, protection rings are a hierarchical model where different levels of the operating system and applications are given different levels of access to the hardware. At the innermost level, or Ring 0, we have the most trusted components—usually the operating system’s kernel. At outer rings, like Ring 3, we have less trusted components, such as user applications.

The idea is to limit what each level can do, ensuring that less trusted software (like user programs) cannot directly interact with or interfere with core system components, such as the kernel or hardware. This separation is what we call isolation. It ensures that if a process in an outer ring, like a user application, crashes or is compromised, it can’t affect more critical parts of the system, like the kernel.

Protection rings are hardware-enforced by the CPU. This means that the CPU itself is designed to manage access control between these rings. The CPU microcode, which is a set of low-level instructions built into the processor, controls what each ring can access. This hardware-enforced model ensures that even if an application tries to access resources outside of its ring (like trying to control hardware directly), the CPU will block that access.

So why are protection rings important? They provide a layer of security and stability for the system:

Security: By preventing applications from directly accessing critical system components, we reduce the risk of vulnerabilities or attacks that could compromise the system.
Stability: If an application crashes or has a bug, it’s contained within its ring, preventing it from affecting the kernel or other critical components.
To summarize:

Protection rings help safeguard the system by isolating different levels of access, protecting the core functionality from less trusted software.
This model is hardware-enforced by the CPU, ensuring that unauthorized access is blocked at the hardware level, thanks to the CPU’s microcode.
This layered security model is key to modern operating system design, ensuring both security and system stability.

P
Kernel Mode vs. User mode​
To protect critical operating system data, modern OS uses two processor access modes: user mode and kernel mode

User application code runs in user mode (ring 3),
OS Kernel (including some system services and device drivers) runs in kernel mode
Kernel mode refers to a processor mode that grants access to all system memory and all CPU instructions (ring 0)​
Note: Some CPU architectures support more protection rings, but only two are used.​ This is valid for all modern general purpose operating systems.

When we talk about operating system security and stability, one of the core concepts is the distinction between Kernel Mode and User Mode. This separation is critical to protecting the operating system’s most sensitive components.

To protect critical system data, modern operating systems use two main processor access modes:

User Mode, where regular user applications run, and
Kernel Mode, where the OS kernel and critical system services operate.
Let’s break it down:

User Mode is the more restricted environment, typically associated with ring 3 in the CPU’s protection ring model. This is where user applications run—things like web browsers, word processors, and games. In user mode, applications have limited access to system resources. They can’t directly interact with hardware or critical system data, which helps to protect the OS from accidental or malicious changes. If an application crashes or misbehaves, it’s isolated in user mode and won’t impact the overall system stability.
On the other hand, the Kernel Mode is the privileged environment, corresponding to ring 0 in the CPU’s hierarchy. This is where the operating system kernel operates, along with certain system services and device drivers that need full access to the system’s hardware. In kernel mode, processes can access all system memory and execute any CPU instructions. Because of this unrestricted access, anything running in kernel mode can make significant changes to the system. However, this also means that errors or crashes in kernel mode can cause system-wide problems.
This two-mode model is a key part of system protection. It ensures that regular applications can’t interfere with critical system functions, while still allowing the kernel to manage resources and control hardware.

Note: While some CPU architectures support more than two protection rings (such as rings 1 and 2), only two—user mode (ring 3) and kernel mode (ring 0)—are commonly used in all modern general-purpose operating systems like Windows, Linux, and macOS. This simplified model balances security and performance while keeping the system stable.

In summary:

User Mode (ring 3) is where user applications run with limited access to protect the system.
Kernel Mode (ring 0) is where the operating system kernel and critical components run with full access to hardware and system resources.
This separation is crucial for protecting critical OS data and ensuring the stability of the overall system.

P
Module or Driver
Device

Unit of hardware that performs a special function and is attached to a computer.
Kernel module or Device Driver

Small software program that operates or controls a particular type of device that is attached to a computer
Manages and translates user I/O function calls into specific hardware device I/O requests
Now, let’s talk about devices and how the operating system interacts with them using kernel modules or device drivers.

First, what is a device? A device is a unit of hardware that performs a specific function and is attached to the computer. This could be anything from a keyboard, printer, or mouse to more complex devices like hard drives, network adapters, or graphics cards. Each of these devices has its own specific function and needs a way to communicate with the computer system.

That’s where kernel modules or device drivers come into play.

A kernel module or device driver is a small software program that operates or controls a particular type of hardware device attached to the computer. The key role of a device driver is to act as a translator between the operating system and the hardware. Since the operating system itself doesn’t directly communicate with hardware devices, the driver serves as the middle layer, allowing the OS to send instructions and receive data from the hardware.

For example, when you click the print button in an application, the operating system doesn’t directly send print commands to the printer. Instead, it makes a call to the device driver, which translates that command into a format that the printer can understand. Similarly, when the printer is ready to send data back to the system (such as a status report), it communicates through the driver, which translates that data into something the operating system can process.

These drivers manage and translate user input/output (I/O) function calls into specific hardware I/O requests. This is a crucial function because each hardware device communicates differently, and the driver ensures that the operating system doesn’t need to know the exact details of how the hardware operates. All the complexity is handled by the driver, providing a smooth and uniform way for the OS to manage hardware.

To summarize:

A device is a unit of hardware that performs a specific function and is attached to the computer.
A kernel module or device driver is a small software program that controls a specific device.
It acts as a translator, managing user I/O requests and converting them into the specific instructions the hardware needs to operate.
This interaction is vital for the smooth operation of hardware in any computer system, allowing users to interact with devices seamlessly without needing to understand how the hardware works internally.

P
How OS identifies which driver to load/use?
Device identification string

The computer devices have a set of registers that identify the vendor and the device model
The OS uses those IDs to identify the attached devices and to select the suitable drivers for them
Example of device identification string

PCI VEN_10E8&DEV_4750
When you connect a new device to your computer, such as a printer, graphics card, or network adapter, how does the operating system know which driver to load for that device? This process relies on something called the device identification string.

Every hardware device has a set of unique identifiers stored in registers on the device. These identifiers are typically composed of a vendor ID and a device model ID. Together, they form what we call the device identification string. This string is unique to each type of device and allows the operating system to recognize exactly what hardware is connected.

For example, a device might have an identification string like PCI VEN_10E8&DEV_4750. In this case:

VEN_10E8 is the Vendor ID, identifying the company that manufactured the device.
DEV_4750 is the Device ID, identifying the specific model of the device.
When you connect the device, the operating system reads these IDs from the device’s registers. Using this information, the OS searches through its database of drivers to find the one that matches this specific device. Once the correct driver is found, the OS loads it, allowing the device to function properly and communicate with the system.

So, in summary, the device identification string plays a crucial role in ensuring the correct drivers are loaded:

Devices have built-in identifiers (vendor and device IDs) stored in their hardware registers.
The OS reads these IDs to identify the device.
The OS then uses the IDs to find and load the appropriate driver, ensuring the device operates correctly.
This automated process ensures that your system can recognize and configure new hardware without manual intervention, making the user experience smoother

P
Server vs Desktop OS
Server Operating Systems:

Designed to manage and serve multiple users and systems
Focus on stability, security, and resource management
Optimized for background tasks and services (e.g., databases, web servers)
Examples: Linux (Ubuntu Server, RedHat Enterprise Linux, Debian, etc.), Windows Server, Unix-based systems (BSD, AIX, etc.)
Desktop Operating Systems:

Designed for personal or single-user tasks
Focus on user interface, ease of use, and multimedia
Optimized for interactive tasks (e.g., browsing, document editing)
Examples: Windows, MacOS, Linux (Ubuntu Desktop, Fedora, etc.)
When we talk about operating systems, it’s important to understand the differences between server and desktop operating systems, as they serve different purposes and are optimized for distinct use cases.

First, let’s look at Server Operating Systems. These are designed primarily to manage and serve multiple users and systems simultaneously. Their main focus is on providing stability, security, and resource management in environments where uptime is critical. For example, a server OS might run web servers, databases, or application services that need to be accessible to users around the clock. Server operating systems are optimized for background tasks, where performance, efficiency, and multi-user support are key. Examples of server operating systems include Linux distributions like Ubuntu Server, RedHat Enterprise Linux and Debian, Windows Server, and Unix-based systems like BSD.

On the other hand, Desktop Operating Systems are designed for personal or single-user tasks. The primary focus here is on providing an easy-to-use, interactive experience for the user. Desktop OSs are optimized for activities like browsing the web, editing documents, or running multimedia applications. Because desktop users are interacting with the system directly, user interface and ease of use are critical design priorities. Examples of desktop operating systems include Windows, macOS, and desktop versions of Linux like Ubuntu Desktop or Fedora.

In short:

Server OS: Designed for running background tasks, serving multiple users, and ensuring stability and performance in a multi-user environment.
Desktop OS: Designed for single users, focusing on user-friendly interfaces and handling tasks that require direct user interaction.
These differences are important to keep in mind, as each OS type is optimized for its specific role.


P
Modern server operating system paradigms
Automatic Provisioning
Orchestration
Configuration Managemen
Self-Healing
Immutability
Infrastructure as Code (IaC)
Let’s take a look at some of the most important modern paradigms in server operating system management. These trends have revolutionized how servers are deployed, managed, and maintained in modern infrastructures.

Automatic Provisioning

Automatic provisioning is the process of automating the setup and deployment of servers and infrastructure. Instead of manually configuring each server, tools like Ansible and Terraform allow you to define server templates and automatically deploy them. This speeds up deployment times, ensures consistency, and reduces the chances of human error.
Orchestration

As systems grow in complexity, managing multiple servers and services manually becomes difficult. Orchestration automates the coordination of these servers and services to ensure they work together efficiently.
Configuration Management

Configuration management is all about maintaining consistent server configurations across environments. Tools like Ansible allow you to define the desired state of your infrastructure and automatically apply those settings to servers, ensuring that all systems are correctly configured and that configuration changes are tracked and applied consistently.
Self-Healing

Self-healing systems are designed to automatically detect and recover from failures without human intervention. For example, Kubernetes can automatically restart failed application or reschedule workloads when a server goes down. This reduces downtime and helps maintain system resilience, ensuring that services remain available even during unexpected issues.
Immutability

The concept of immutability means that servers are not modified after deployment. Instead of applying patches or updates to a running system, an immutable infrastructure replaces entire servers or containers with new instances whenever changes are needed. This prevents configuration drift and ensures that systems remain consistent over time. Docker and Kubernetes are commonly used to implement immutable infrastructures.
Infrastructure as Code (IaC)

Infrastructure as Code (IaC) is the practice of managing and provisioning infrastructure through code, rather than manual processes. This allows infrastructure to be version-controlled and treated just like software code, enabling teams to collaborate and apply changes in a controlled, repeatable way. Tools like Terraform or OpenTofu make it easy to define infrastructure in code and automatically deploy it to production environments.
In summary, modern server operating systems are no longer managed manually. Instead, they rely on automation, orchestration, and self-healing mechanisms to ensure that systems are scalable, resilient, and easy to manage. These paradigms make it possible to handle complex infrastructure in a consistent, efficient, and secure way.”