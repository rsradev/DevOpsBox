Module 3 - Fundamental of Networking

1 / 57
Goto...
P
Network Components
Data
Client
Server
Peer
Network adapter
Network Media
Network Devices
Let’s explore the fundamental components that make up a computer network. These components work together to allow devices to communicate, share resources, and transfer data efficiently. Understanding these key components is essential for grasping how networks operate.

Data
At the heart of any network is data—the information that is being transferred between devices. Whether it’s a file, a video stream, or an email, the goal of a network is to facilitate the movement of data from one device to another. Data is broken down into smaller units, such as packets, to ensure efficient transmission across the network.

Client
A client is a device or software application that requests services or resources from another device, usually a server. For example, when you open a web browser and visit a website, your computer acts as the client, requesting data from the server hosting the site. Clients rely on the network to communicate with servers and retrieve the necessary data.

Server
A server is a device or software that provides services or resources to clients. Servers host data, applications, or services, such as websites, databases, or files, and respond to client requests. The server plays a central role in most networked environments by managing resources and ensuring that clients have access to what they need.

Peer
In some networks, especially in peer-to-peer (P2P) models, devices can act as both clients and servers. These devices are referred to as peers. Unlike the traditional client-server model, where servers provide services and clients request them, peers in a P2P network can share resources directly with one another without needing a centralized server. This model is commonly used in file-sharing applications and decentralized networks.

Network Adapter
A network adapter is the hardware that enables a device, such as a computer or a smartphone, to connect to a network. This can be in the form of a wired adapter, like an Ethernet card, or a wireless adapter, like a Wi-Fi card. The network adapter facilitates communication between the device and the network by transmitting and receiving data.

Network Media
Network media refers to the physical means through which data travels from one device to another. This can include wired media, like Ethernet cables (twisted pair, coaxial, fiber-optic), or wireless media, such as radio waves used in Wi-Fi. The type of network media used determines the speed and reliability of the network connection.

Network Devices
Network devices include hardware components like routers, switches, hubs, and firewalls that help manage data traffic and ensure the efficient transfer of data across the network. These devices are critical for directing traffic, maintaining security, and ensuring that data reaches its intended destination.

In summary, a computer network consists of various components, each playing a specific role in facilitating communication. Data is the core element being transferred, while clients and servers request and provide resources. Peers act as both in certain models, and network adapters and network media enable devices to connect and communicate. Lastly, network devices manage, direct, and secure the flow of data across the network, ensuring everything runs smoothly.


2 / 57
Goto...
P
Server vs Client
Server

Computer that provides shared resources and serves client requests
Client

Computer that sends requests to other computers in the network

Let’s now explore the relationship between servers and clients in a networked environment. These two roles are fundamental to how most networks operate.

Server
A server is a computer or software system that provides shared resources and serves client requests. In a network, the server is responsible for managing resources such as files, databases, websites, or applications, and delivering them to the clients that request access. Servers are typically more powerful machines, optimized for handling multiple requests simultaneously and maintaining data integrity.

For example, when you visit a website, the web server hosting the site processes your request and delivers the website content to your browser. Other types of servers include file servers for managing file storage and access, database servers for handling database queries, and mail servers for managing email communication. The key role of the server is to act as the central hub that provides services to multiple clients in a network.

Client A client is a computer or software system that sends requests to other computers, specifically to a server, in the network. Clients initiate communication by requesting services or resources from the server. The client relies on the server to provide the data or functionality it needs, whether it’s accessing a webpage, retrieving a file, or querying a database.

For example, when you open your email application, your device acts as a client, sending a request to the mail server to retrieve your messages. The server responds by delivering the requested data, which the client then processes and displays for you. Clients can be anything from a desktop computer or mobile device to software applications that interact with remote servers.

In summary:

A server is a centralized system that provides resources and handles multiple requests from clients.
A client is a system that requests resources or services from a server. This client-server model is the foundation of most networked systems, enabling efficient sharing of resources and services across multiple devices.

3 / 57
Goto...
P
OSI vs TCP/IP

4 / 57
Goto...
P
OSI Model
Open Systems Interconnection model
Conceptual/Reference model
Defines generic network communication processes
Layers communicate (pass information) with the layers above and below them
Gives people “jargon” with which to talk about data processing over a network

The OSI Model (Open Systems Interconnection model) is a conceptual framework that helps us understand how network communication happens between different systems. It’s not a physical implementation but a reference model that defines how different software and hardware components communicate over a network. Let’s break down its key components and purpose:

Conceptual/Reference Model
The OSI Model serves as a conceptual guide. It was developed to standardize network communication processes, making it easier for different technologies and systems to interoperate. This model doesn’t directly represent any single technology or protocol but instead provides a blueprint for how network communication should be structured.

Defines Generic Network Communication Processes
The OSI Model breaks down the process of network communication into seven distinct layers. Each layer handles a specific part of the communication, such as data formatting, addressing, or physical transmission. By dividing the process into layers, the model helps us understand how data flows through a network and how various devices interact with one another.

Layers Communicate with Layers Above and Below
Each layer of the OSI Model has a specific role, and they communicate vertically with the layers directly above and below them. For instance, when data moves from one device to another, it passes through all layers of the OSI Model on both ends—starting at the top layer, traveling down through the layers on the sending side, and then back up through the layers on the receiving side. This modular approach ensures that each layer can function independently while still working together as a whole.

Gives People "Jargon" for Talking About Data Processing Over a Network
One of the main benefits of the OSI Model is that it gives us common terminology or “jargon” to discuss how data is processed and transmitted across a network. When networking professionals talk about issues at the network layer or the application layer, they are referring to specific layers within the OSI Model. This shared language helps professionals across different industries and vendors to troubleshoot, design, and optimize network systems more effectively.

Looking at the seven layers in the image:

Layer 1: Physical Layer – Deals with the physical connection between devices.
Layer 2: Data Link Layer – Ensures data is transferred error-free from one node to another.
Layer 3: Network Layer – Handles addressing and routing of data packets.
Layer 4: Transport Layer – Ensures complete data transfer with error checking and flow control.
Layer 5: Session Layer – Manages sessions between applications.
Layer 6: Presentation Layer – Translates data into a readable format for applications.
Layer 7: Application Layer – The closest layer to the user, interacting directly with software applications.
In summary, the OSI Model helps break down network communication into seven layers, providing a structured and standardized way to understand and discuss how data flows through a network. This layered approach simplifies the complexity of networking and makes troubleshooting and system design easier.


5 / 57
Goto...
P
TCP/IP Model
Transmission Control Protocol/Internet Protocol
Developed prior to the OSI model
Developed by Department of Defense (DoD) in 1960s
Based on standard protocols
The “de facto” standard

Let’s explore the TCP/IP Model, which is another important framework used to describe network communication. While the OSI Model is a conceptual model, the TCP/IP Model is the practical standard for how communication happens on the internet today.

Transmission Control Protocol/Internet Protocol
The TCP/IP Model stands for Transmission Control Protocol/Internet Protocol, which is the suite of communication protocols used to connect devices over the internet and other networks. TCP/IP governs how data is sent, received, and routed across networks, ensuring reliable and accurate transmission.

Developed Prior to the OSI Model
Interestingly, the TCP/IP Model was developed before the OSI Model, even though we often discuss the OSI Model first in networking. TCP/IP was created out of necessity for practical communication, while the OSI Model is more theoretical and was developed later as a way to standardize network communication concepts.

Developed by the Department of Defense (DoD) in the 1960s
The TCP/IP protocol suite was originally developed in the 1960s by the Department of Defense (DoD) for the ARPANET, which was the precursor to the modern internet. The focus was on building a reliable, robust network that could withstand failures and continue to communicate between systems.

Based on Standard Protocols
TCP/IP is based on standard protocols that ensure interoperability between different devices and networks. For example, TCP ensures reliable data transfer, breaking data into packets and reassembling them at the destination, while IP handles addressing and routing packets to their destination.

The “De Facto” Standard
While the OSI Model is an important conceptual framework, TCP/IP is the “de facto” standard for network communication in today’s world. It’s the protocol suite that powers the internet and is used by almost every networked device globally. The TCP/IP model is simpler and more streamlined than the OSI Model, making it highly practical and widely adopted.

Looking at the mapping between the OSI and TCP/IP models in the image, you can see that the TCP/IP Model has fewer layers:

The Application layer in TCP/IP combines the Application, Presentation, and Session layers from the OSI Model. This means that these functions are handled together in the TCP/IP model.
The Transport layer in both models serves a similar purpose, ensuring reliable data transfer.
The Internet layer in TCP/IP maps to the Network layer in OSI, dealing with packet routing and addressing.
The Network Access layer in TCP/IP combines the Data Link and Physical layers from OSI, managing the physical transmission of data over the network.
In summary, the TCP/IP Model is the practical foundation for internet communication, with its roots in the 1960s as a DoD project. It’s based on standard protocols like TCP and IP and has become the “de facto” standard for modern networking. This model, although simpler than the OSI model, plays a vital role in ensuring reliable, scalable, and efficient communication across the internet.


6 / 57
Goto...
P
Transport Protocols TCP (Layer 4)
Transmission Control Protocol (TCP)

Connection oriented
Establish connection before data can be sent
Uses Three-way handshake
Reliable and Error-checked
Error detection algorithm
Order algorithm
Now let’s dive into TCP, or Transmission Control Protocol, which is one of the most important protocols in the Transport Layer (Layer 4) of both the OSI and TCP/IP models. TCP is widely used because it provides reliable communication between devices over a network.

Connection-Oriented
TCP is a connection-oriented protocol, which means that before any data is transmitted, a connection must first be established between the sender and the receiver. This is done through a process called the Three-way handshake.

Here’s how the Three-way handshake works:

Step 1: The sender initiates the process by sending a SYN (synchronize) packet to the receiver.
Step 2: The receiver responds with a SYN-ACK (synchronize-acknowledge) packet, confirming the request.
Step 3: The sender then sends an ACK (acknowledge) packet, and the connection is established. This handshake ensures that both the sender and receiver are ready to communicate, making TCP a reliable and structured protocol for data transmission.
Reliable and Error-Checked
One of the key advantages of TCP is that it ensures reliable transmission of data. TCP uses error detection algorithms to ensure that data is delivered accurately, without corruption. When data is transmitted, TCP breaks it into smaller packets, and each packet is checked for errors during transmission. If any packet is lost or corrupted, TCP ensures it is retransmitted until it is received correctly.

TCP also includes an order algorithm, which ensures that data packets are received in the correct order, even if they arrive out of sequence. For example, if a series of packets arrives at the destination in the wrong order, TCP reassembles them into the correct sequence before passing them to the application layer. This ordering mechanism guarantees that data arrives as it was intended.

In summary, TCP provides two essential features:

Error-checking: By using checksums and acknowledgments, TCP ensures that corrupted or lost data is retransmitted.
Ordering: TCP ensures that packets are delivered and reassembled in the correct sequence, even if they arrive out of order.
In conclusion, TCP is a connection-oriented, reliable, and error-checked transport protocol that ensures data is delivered accurately and in the right order. This makes it ideal for applications where data integrity and reliability are critical, such as web browsing, email, and file transfers.

7 / 57
Goto...
P
Transport Protocols UDP (Layer 4)
User Datagram Protocol (UDP)

Simple
Minimum of protocol mechanisms
Minimum latency
Not connection oriented
Good for applications that do not require reliable data stream service
Application developers must implement reliability on application layer
Now let’s talk about UDP, or User Datagram Protocol, which is another important protocol in the Transport Layer (Layer 4). While TCP focuses on reliability, UDP prioritizes simplicity and speed, making it suitable for different kinds of applications.

Simple
UDP is a very simple protocol. It has a minimum set of protocol mechanisms, which means there’s very little overhead in terms of how data is processed and transmitted. Because UDP doesn’t have the complexity of TCP’s connection setup, reliability checks, or order enforcement, it has lower latency, making it faster in certain scenarios. This makes UDP ideal for applications where speed is more important than reliability.

For example, real-time applications like live video streaming, online gaming, and voice-over-IP (VoIP) benefit from UDP’s minimal latency because these applications can tolerate occasional data loss without requiring retransmissions or ordering guarantees. It ensures the data flows quickly and efficiently, even if some packets are dropped.

Not Connection-Oriented
Unlike TCP, UDP is not connection-oriented, which means there is no need to establish a formal connection between the sender and the receiver before data can be sent. Instead, data is sent as datagrams, without ensuring that the receiver is ready or that the data will be delivered reliably.

This is a key difference
UDP does not handle error checking, retransmissions, or ensuring data arrives in order. If packets are lost or arrive out of order, UDP does not attempt to fix these issues. Therefore, it’s up to the application developer to decide how to handle reliability, error checking, or reordering at the application layer if needed.

UDP is ideal for applications where a reliable data stream isn’t required. For instance, in video streaming, if a few data packets are lost, it won’t significantly impact the user experience, and retransmitting them would cause more delay. Similarly, in online gaming, speed and low latency are crucial, and the game can usually continue functioning even if a few packets are lost.

In summary, UDP offers two key features:

Simplicity and speed: Minimal protocol mechanisms lead to lower latency and faster transmission, which is useful for real-time applications.
No connection-oriented guarantees: It doesn’t ensure reliable transmission, leaving that responsibility to the application, if needed.
In conclusion, UDP is a simple and fast transport protocol best suited for applications where speed and low latency are critical, and reliability is either less important or can be handled at the application layer. Examples include video streaming, online gaming, and voice-over-IP.

8 / 57
Goto...
P
Sockets and Ports
Combination of protocol, port number, source and destination IP address


Now let’s talk about sockets and ports, two key concepts that help manage network communication between devices.

What is a Socket?
A socket is a combination of four elements:

Protocol (e.g., TCP or UDP)
Port number
Source IP address
Destination IP address
Think of a socket as a communication endpoint that allows two devices to exchange data. A socket uniquely identifies a connection between a client and a server over the network. The combination of these elements ensures that data can be sent and received by the correct application on the correct device.

Ports
Ports are numerical values that help identify specific services or applications running on a device. For example:

Port 80 is commonly used for HTTP (web traffic).
Port 443 is used for HTTPS (secure web traffic).
Port 25 is used for SMTP (email). Each application that wants to communicate over the network uses a specific port number, allowing multiple applications on the same machine to use the network simultaneously. This prevents data from being sent to the wrong application.
How Sockets Work
The socket works by combining the protocol, port number, source IP address, and destination IP address to establish a communication link.

For example, when a client device (like your laptop) communicates with a web server, the socket includes:

The TCP protocol (since web traffic typically uses TCP),
A port number (e.g., port 80 for HTTP),
The source IP address (your laptop’s IP),
The destination IP address (the web server’s IP).
This combination ensures that the data you request (such as a webpage) is sent to the correct server and that the server’s response is routed back to the right application on your device. The port number tells the device which application (e.g., web browser) should handle the incoming data, while the IP addresses identify the devices that are sending and receiving the data.

The image on the slide illustrates this concept by showing how a socket ties all these elements together to facilitate communication between two devices.

In summary, sockets are essential for managing network communication by combining the protocol, port number, source IP, and destination IP. This combination ensures that data is sent to the right place and received by the correct application, making it possible for multiple devices and applications to communicate over a network simultaneously.


9 / 57
Goto...
P
Well known application protocols
HTTP
SMTP
SMTP Server to SMTP Server delivery
SMTP Client to SMTP Server delivery
POP3
IMAP
DNS
DHCP
FTP
SSH
RDP
VNC
NTP
Now, let’s discuss some of the most widely used application protocols in networking. These protocols are responsible for facilitating communication between devices and services over a network, and each one serves a specific purpose.

HTTP (Hypertext Transfer Protocol)
HTTP is the foundation of data communication on the World Wide Web. It’s used for transmitting web pages from web servers to web browsers. Every time you visit a website, your browser is making HTTP requests to retrieve and display the content. Example: http:// websites use this protocol.

SMTP (Simple Mail Transfer Protocol)
SMTP is used for email transmission. It enables the sending of email messages between SMTP servers or between an SMTP client (such as an email application) and an SMTP server.

SMTP Server to SMTP Server Delivery: This is the process by which email is transferred between mail servers.
SMTP Client to SMTP Server Delivery: This handles sending mail from a client (like Outlook or Gmail) to the server, which then forwards it to the recipient’s mail server.
POP3 (Post Office Protocol 3)
POP3 is used for retrieving email from a mail server to a client. Once the email is downloaded, it is typically deleted from the server. This protocol is simple but not ideal for users who need to access their mail from multiple devices.

IMAP (Internet Message Access Protocol)
IMAP is a more advanced protocol for retrieving email, offering more flexibility than POP3. It allows users to synchronize their email across multiple devices, leaving copies of messages on the server. This means you can access your emails from different devices, and all changes are reflected across them.

DNS (Domain Name System)
DNS translates domain names (like www.example.com) into IP addresses. Since computers use IP addresses to communicate, DNS acts as the internet’s phonebook, allowing users to connect to websites using human-readable names instead of complex numerical IP addresses.

DHCP (Dynamic Host Configuration Protocol)
DHCP automatically assigns IP addresses to devices on a network. When a device connects to a network, DHCP ensures that it receives an IP address and other necessary network configuration settings, such as the default gateway and DNS server.

FTP (File Transfer Protocol)
FTP is used for transferring files between computers over a network. It’s commonly used to upload files to a web server or download files from a server. FTP can be accessed using an FTP client or through a web browser in some cases.

SSH (Secure Shell)
SSH provides a secure method for remote login and command execution over a network. It’s often used by system administrators to access and manage servers securely. SSH encrypts all data transferred between the client and server, ensuring a secure connection.

RDP (Remote Desktop Protocol)
RDP is a protocol developed by Microsoft that allows users to remotely control another computer over a network. It’s commonly used for remote access to Windows-based systems, enabling users to view and interact with a remote desktop as if they were sitting in front of it.

VNC (Virtual Network Computing)
Similar to RDP, VNC allows for remote control of a computer over a network. However, VNC is platform-independent, meaning it can be used across different operating systems (e.g., controlling a Linux machine from a Windows device).

NTP (Network Time Protocol)
NTP is used to synchronize the clocks of devices over a network. Keeping accurate time is essential for various network tasks, such as logging events, authenticating users, and coordinating scheduled jobs.

In summary, these well-known application protocols are critical for enabling communication and data exchange across the internet and internal networks. From web browsing and email communication to file transfers and remote access, these protocols facilitate many of the tasks we perform on networks every day.


10 / 57
Goto...
P
IP Addressing

11 / 57
Goto...
P
What is an IP Address?
Internet Protocol Address
Numerical label
Each device in TCP/IP network has an IP address
Displayed in human-readable notation
IPV4 - 172.16.254.1
IPV6 - 2001:db8:0:1234:0:567:8:1 (human-readable :D)
IP versions
IPV4 (32 bits)
First deployed in 1983 (ARPANET)
IPV6 (128 bits)
First deployed in ~2000
Today, we use both versions of the IP simultaneously
Let’s talk about IP addresses, which are essential for identifying devices on a TCP/IP network. Every device that communicates over a network needs a unique IP address to ensure data is sent and received correctly.

Numerical Label
An IP address is a numerical label assigned to each device (such as computers, servers, smartphones, etc.) connected to a TCP/IP network. This address uniquely identifies the device and allows it to communicate with other devices on the network, whether on a local network or over the internet.

Displayed in Human-Readable Notation
IP addresses are displayed in a human-readable format, which varies depending on whether it’s an IPv4 or IPv6 address.

IPv4: An IPv4 address is written as a series of four numbers, separated by periods, like this: 172.16.254.1. Each number represents 8 bits, making a total of 32 bits.
IPv6: With the exhaustion of IPv4 addresses, IPv6 was introduced. An IPv6 address is significantly longer, written as a series of eight groups of four hexadecimal digits separated by colons, like this: 2001:db8:0:1234:0:567:8:1. While it may seem complex, this format ensures that we have more than enough IP addresses to meet global demand.
IP Versions
There are two main versions of IP addresses in use today:

IPv4 (32 bits): IPv4 was the first widely deployed version of IP, introduced in 1983 as part of the ARPANET, which was the early predecessor to the internet. Since it uses a 32-bit address, IPv4 allows for around 4.3 billion unique addresses. With the explosion of internet-connected devices, the pool of available IPv4 addresses has become limited.
IPv6 (128 bits): To address the limitations of IPv4, IPv6 was introduced around the year 2000. With a 128-bit address, IPv6 can support an almost infinite number of addresses—approximately 340 undecillion unique IP addresses. This ensures that every device on the planet can have a unique IP address far into the future.
Today, We Use Both Versions Simultaneously
Even though IPv6 is the newer standard, IPv4 is still widely used. In fact, today, most networks use both versions simultaneously. Many modern devices and networks are dual-stack, meaning they can handle both IPv4 and IPv6 traffic, ensuring compatibility and a smooth transition as IPv6 adoption grows.

In summary, an IP address is a unique numerical label that identifies a device on a network. We use two versions of IP: IPv4, which is still dominant, and IPv6, which is being gradually adopted. Both of these versions are critical for ensuring that devices can communicate and exchange data across the internet.


12 / 57
Goto...
P
IANA
Internet Assigned Numbers Authority (IANA)

Global standardization organization (Located in US)
Oversees global IP address allocation

IPV4 and IPV6
IANA delegates allocations of IP address blocks to regional Internet registries
Each RIR allocates addresses for a different area of the world
IANA manages the global registry of the BGP Autonomous System Numbers (ASNs)
Manages the data in the root DNS nameservers

No more IPV4 public addresses…

Let’s now discuss the Internet Assigned Numbers Authority (IANA), a global organization that plays a critical role in managing and standardizing key elements of the internet.

Global Standardization Organization
The IANA is responsible for overseeing many of the global standards that keep the internet running smoothly. Based in the United States, IANA is a vital part of the Internet Corporation for Assigned Names and Numbers (ICANN). Its primary role is to ensure that IP addresses and domain names are managed in a way that promotes global interoperability and coordination.

Oversees Global IP Address Allocation
One of IANA’s main responsibilities is to oversee the allocation of IP addresses for both IPv4 and IPv6. However, IANA doesn’t allocate individual IP addresses directly to users or organizations. Instead, it delegates the allocation of IP address blocks to Regional Internet Registries (RIRs).

IPv4 and IPv6: IANA manages the central pool of IP addresses and distributes large blocks of addresses to the RIRs.
Regional Internet Registries (RIRs): There are five RIRs, each responsible for different geographic regions of the world. These RIRs, such as ARIN for North America and RIPE NCC for Europe, handle the allocation of IP addresses to internet service providers (ISPs) and organizations in their respective regions.
Manages the Global Registry of BGP Autonomous System Numbers (ASNs)
Another key responsibility of IANA is managing the global registry of BGP Autonomous System Numbers (ASNs). These numbers are used to identify autonomous systems that make up the global routing infrastructure of the internet. Autonomous systems help manage the large-scale routing of data across multiple networks, and IANA ensures that ASNs are allocated and maintained properly to avoid conflicts.
Manages the Data in the Root DNS Nameservers
IANA is also responsible for managing the root zone of the Domain Name System (DNS). This is crucial because DNS is what translates human-readable domain names (like www.example.com) into the numerical IP addresses that computers use to communicate. The root DNS nameservers form the foundation of this system, and IANA ensures that they are kept up to date and secure.

No More IPv4 Public Addresses…
One of the major developments in recent years is that IANA has exhausted its supply of IPv4 addresses. The rapid growth of the internet has used up the available pool of public IPv4 addresses. This is one of the reasons why the transition to IPv6 is so important. IPv6 offers a much larger pool of addresses, ensuring that the internet can continue to grow without running into address shortages.

In summary, the Internet Assigned Numbers Authority (IANA) plays a vital role in global IP address allocation, managing the root DNS nameservers, and overseeing the BGP Autonomous System Numbers (ASNs). As the internet continues to evolve, IANA ensures that critical infrastructure like IP addresses and DNS remain standardized and available to everyone.


13 / 57
Goto...
P
IPV4 Addresses
32 Bits
11000000101010000001111000101000
4 groups of 8 bits (Octet)
11000000.10101000.00011110.00101000
Decimal base
192.168.30.40
Let’s dive into IPv4 addresses, one of the most widely used formats for identifying devices on a network. Even though we’re transitioning toward IPv6, IPv4 remains important for understanding network addressing.

32 Bits
An IPv4 address is a 32-bit number. This means it is made up of 32 individual bits, which are either 1s or 0s. When written in its binary form, an IPv4 address looks something like this: 11000000101010000001111000101000

However, reading or interpreting an IPv4 address in binary form would be difficult for humans, so we use other formats to make it easier to understand.

4 Groups of 8 Bits (Octet)
To make IPv4 addresses more manageable, they are broken down into 4 groups of 8 bits. Each of these 8-bit groups is referred to as an octet. For example, in the binary form: 11000000.10101000.00011110.00101000
By separating the 32 bits into four octets, we can more easily work with and understand the address. Each octet represents a segment of the overall address, and the dots between the octets help define the boundaries of each group.

Decimal Base (Dotted Decimal Notation) Since binary is not easy to read, IPv4 addresses are typically displayed in a decimal format known as dotted decimal notation. Each octet is converted from binary to a decimal value, and then the four decimal values are separated by dots.

For example: 11000000.10101000.00011110.00101000 (binary) becomes 192.168.30.40 (decimal)

This is the form we most commonly see and use. Each decimal value in an octet can range from 0 to 255, corresponding to the 8 bits of binary information. This format is more human-readable and easier to work with when configuring networks or diagnosing issues.

In summary, an IPv4 address is a 32-bit number that is broken down into 4 octets of 8 bits each. While the address is technically binary, we typically express it in dotted decimal notation for easier reading and use. For example, the binary address 11000000.10101000.00011110.00101000 becomes 192.168.30.40 in decimal form.


14 / 57
Goto...

IPV4 Calculation

P
Subnet mask (IPV4)
Also known as netmask
Solves some of the problems of classful addressing
Consists of 32 bits
Sequence of ones (1) followed by a block of zeros (0)
Ones - indicate bits used for the network prefix
Zeros - designates the host identifier

CIDR alternate method of representing subnet mask
Count of the number of network bits prefixed with “/” – 192.0.2.130/24
Now let’s discuss the subnet mask, which plays a crucial role in IPv4 addressing by determining how an IP address is divided into network and host portions.

Also Known as Netmask
The subnet mask, also referred to as a netmask, is used to identify which part of an IP address refers to the network and which part identifies the specific device (host) within that network. Without a subnet mask, the IP address wouldn’t be able to differentiate between the network and host portions.

Solves Problems of Classful Addressing
Subnet masks help solve issues with the old classful addressing system. In classful addressing, the network and host portions of an address were rigidly defined based on the first few bits of the IP address. This led to inefficient use of IP addresses and limited flexibility. The subnet mask allows for more granular control, enabling administrators to define custom-sized networks that better fit their needs.

32 Bits
Like an IPv4 address, the subnet mask is made up of 32 bits. However, the way these bits are arranged helps to distinguish the network part from the host part of the address.

Sequence of Ones (1) Followed by Zeros (0)
The subnet mask is typically represented as a sequence of ones (1s) followed by a block of zeros (0s):

The ones (1s) indicate the network prefix, which shows how many bits in the IP address are used to identify the network.
The zeros (0s) designate the host identifier, which represents the specific device on the network.
For example, the subnet mask 255.255.255.0 in binary would look like this: 11111111.11111111.11111111.00000000
The first 24 bits (ones) identify the network, and the remaining 8 bits (zeros) are used to identify the host within that network.
CIDR (Classless Inter-Domain Routing)
CIDR is an alternate method of representing a subnet mask, which simplifies the notation. Rather than writing out the full subnet mask in decimal or binary, CIDR uses a shorthand to indicate the number of bits in the network portion of the address. This is done by appending a “/” followed by the number of network bits.

For example, 192.0.2.130/24 means that the first 24 bits of the IP address are the network portion, leaving the remaining 8 bits for the host. The /24 is much easier to use than writing out the full subnet mask 255.255.255.0.

In summary, the subnet mask is a 32-bit sequence that helps divide an IP address into the network prefix and host identifier. It plays a critical role in ensuring efficient IP address allocation by solving issues with classful addressing. The CIDR notation is a more convenient way of representing subnet masks, making network management more flexible and scalable.


16 / 57
Goto...

P
IPV6
128 bits 🛈
8 groups of 16 bits 🛈
HEX base 16 bits 🛈
Removing leading zeros 🛈
No subnet mask
Prefix Length or just prefix
Similar to CIDR in IPV4
No broadcast
Now, let’s explore IPv6, the next-generation Internet Protocol designed to replace IPv4 and solve the issue of address exhaustion, as well as provide enhanced capabilities.

128 Bits
An IPv6 address is made up of 128 bits, which is significantly larger than the 32-bit structure of IPv4. This gives us an almost unlimited supply of IP addresses, with enough capacity to assign unique addresses to every device on the planet many times over. The binary representation of an IPv6 address would look something like this: 00100000000000010000110110111000000000000000000000101111001110110000001010101010000000001111111111111110001010001001110001011010

8 Groups of 16 Bits

IPv6 addresses are broken down into 8 groups of 16 bits (instead of IPv4’s 4 octets). These groups are separated by colons to make the address more readable. For example, the binary representation of an IPv6 address looks like this: 0010000000000001 0000110110111000 0000000000000000 0010111100111011 0000001010101010 0000000011111111 1111111000101000 1001110001011010
However, IPv6 addresses are usually written in hexadecimal for easier reading.
Hexadecimal (Base-16)

Each group of 16 bits is converted into hexadecimal (base-16) format. In hexadecimal, an IPv6 address might look like this: 2001:0DB8:0000:2F3B:02AA:00FF:FE28:9C5A
The hexadecimal system allows IPv6 addresses to be expressed in a more compact and readable format.
Removing Leading Zeros

To simplify the representation further, we can remove leading zeros from each group. This makes the address even shorter and easier to work with. For example: 2001:0DB8:0000:2F3B:02AA:00FF:FE28:9C5A becomes 2001:DB8:0:2F3B:2AA:FF:FE28:9C5A
This compact format is often seen in day-to-day usage of IPv6 addresses.
No Subnet Mask – Prefix Length

In IPv6, there’s no traditional subnet mask like we see in IPv4. Instead, IPv6 uses a prefix length or simply prefix to represent how many bits are used for the network portion of the address. This works similarly to CIDR notation in IPv4.
For example, 2001:DB8::/32 means the first 32 bits are reserved for the network, while the remaining bits are available for devices on that network.
No Broadcast

Unlike IPv4, IPv6 does not use broadcasts to communicate with all devices on a network. Instead, IPv6 uses multicast and anycast addresses for more efficient and targeted communication. This helps reduce network congestion and improves overall performance.
In summary, IPv6 addresses are 128 bits long, expressed in hexadecimal notation, and broken into 8 groups of 16 bits. Leading zeros can be removed to simplify the format, and instead of a subnet mask, IPv6 uses a prefix length to define the network portion of the address. Additionally, IPv6 eliminates the need for broadcast communication, using more efficient multicast and anycast methods.


17 / 57
Goto...
P
DHCP
Dynamic Host Configuration Protocol
Dynamically assigns network configuration
Reducing the need for manual network administration
From home networks to big enterprise
DORA

Discover package is sent to every device in the network (Broadcast)
Let’s talk about DHCP, or Dynamic Host Configuration Protocol, which is a key protocol used to dynamically assign network configurations to devices on a network.

Dynamic Host Configuration Protocol (DHCP) DHCP is designed to dynamically assign IP addresses and other network configuration settings, such as the default gateway and DNS servers, to devices when they connect to a network. This eliminates the need for manual configuration, making network management easier, especially as networks grow in size.

Reducing the Need for Manual Network Administration
Without DHCP, every device would need to be manually assigned an IP address and network settings, which can be time-consuming and prone to errors, especially in large networks. DHCP simplifies this by automating the process. When a device joins the network, it automatically receives an IP address and other necessary settings from the DHCP server, making network administration more efficient.

Used in Home Networks to Large Enterprises
DHCP is versatile and used in everything from small home networks to large enterprise environments. In a home network, your router typically acts as the DHCP server, assigning IP addresses to devices like laptops, smartphones, and printers. In larger enterprise networks, dedicated DHCP servers handle the configuration of potentially thousands of devices, ensuring they are all properly connected and configured.

DORA Process
The process by which DHCP assigns IP addresses and configurations is known as DORA, which stands for Discover, Offer, Request, Acknowledge. Let’s break down each step, as illustrated in the image on the slide:

Discover: The device that wants to join the network sends a Discover packet as a broadcast message to all devices on the network. This message is asking, “Is there a DHCP server available to assign me an IP address?”
Offer: The DHCP server responds with an Offer packet, proposing an available IP address and other network settings that the device can use.
Request: The device sends back a Request packet to the DHCP server, asking to lease the offered IP address and confirming its interest in the proposed network settings.
Acknowledge: Finally, the DHCP server sends an Acknowledge packet, confirming that the IP address has been successfully leased to the device, and the network configuration is complete.
This process happens quickly and automatically, enabling devices to seamlessly connect to the network without manual intervention.

Discover Packet – Broadcast It’s important to note that the initial Discover packet is sent as a broadcast to the entire network, which means it reaches every device on the network. This allows the DHCP server to identify devices that need an IP address and offer one accordingly.

In summary, DHCP simplifies network management by automatically assigning IP addresses and other settings to devices, reducing the need for manual configuration. The DORA process—Discover, Offer, Request, Acknowledge—ensures that devices can connect to the network quickly and easily, whether in a small home network or a large enterprise environment.


18 / 57
Goto...

P
DNS
Domain Name System

Hierarchical naming structure - www.my.website.example.com

Decentralized architecture

Essential component of the Internet

Main goal

Translates human-readable domain names to the numerical IP addresses
Components

Name servers
Zones
Resolvers
Records - A, PTR, CNAME, MX, NS, More…
Let’s explore DNS, or the Domain Name System, which is one of the most important systems on the internet. DNS makes it possible for us to access websites and services using human-readable domain names instead of hard-to-remember IP addresses.

Domain Name System
DNS is like the phonebook of the internet. Its primary function is to translate human-readable domain names (like www.example.com) into the numerical IP addresses that computers use to locate and communicate with each other.

Hierarchical Naming Structure
DNS uses a hierarchical structure to organize domain names. For example, let’s break down the domain name www.my.website.example.com:

“com” is the top-level domain (TLD).
“example” is the second-level domain.
“website” is a subdomain under “example”.
“my” is another subdomain.
“www” is typically used to identify web servers (host).
This hierarchical structure makes DNS scalable and efficient, as each level can be managed separately.

Decentralized Architecture DNS operates in a decentralized architecture. This means that no single server or authority controls all the DNS data. Instead, DNS is distributed across a network of name servers around the world. Each server is responsible for different parts of the DNS hierarchy, ensuring that the system is robust, scalable, and resistant to failure.

Essential Component of the Internet
DNS is an essential component of the internet. Without it, we’d have to remember IP addresses for every website or service we want to access, which would be nearly impossible. DNS simplifies the process by allowing us to use easy-to-remember domain names.

Main Goal: Translating Domain Names to IP Addresses The main goal of DNS is to translate human-readable domain names (like www.example.com) into the numerical IP addresses that computers use to locate services and devices on the internet. This process happens behind the scenes, making the internet user-friendly and accessible for everyone.

DNS Components DNS is made up of several key components that work together to manage and resolve domain names:

Name Servers: These are the backbone of DNS, responsible for storing DNS records and responding to queries. Different name servers are responsible for different parts of the DNS hierarchy.
Zones: DNS is divided into zones, which are segments of the DNS hierarchy that are managed by different organizations or entities. Each zone contains DNS records for a specific part of the domain name tree.
Resolvers: DNS resolvers are responsible for finding the correct IP address associated with a domain name. They query name servers on behalf of clients (like your computer) to resolve a domain name into an IP address.
Records: DNS uses different types of records to store information about domain names:
A Record: Maps a domain name to an IPv4 address. This is one of the most common types of DNS records.
PTR Record: Performs a reverse DNS lookup, mapping an IP address back to a domain name.
CNAME Record: Stands for Canonical Name and is used to map an alias to the real or canonical domain name. For example, www.example.com could be a CNAME for example.com.
MX Record: Specifies the mail servers responsible for receiving email for a domain.
NS Record: Stands for Name Server and identifies which name servers are authoritative for a particular domain.
More…: There are additional types of records like TXT for storing text data, SRV for specifying service locations, and many others, depending on the requirements of the domain.
In summary, DNS is the system that makes navigating the internet easy by translating domain names into IP addresses. It uses a hierarchical and decentralized architecture to ensure scalability and reliability, and is made up of several components like name servers, zones, resolvers, and various types of DNS records. The result is a system that powers the modern internet, enabling seamless access to websites and services.


19 / 57
Goto...
P
Wired vs Wireless
IEEE
IEEE 802.3 – Ethernet networks
IEEE 802.11 – Wireless local area networks
Let’s now compare wired and wireless networks, two common types of network connections, and explore how they are standardized under the IEEE.

IEEE Standards
The Institute of Electrical and Electronics Engineers (IEEE) develops and maintains standards that define how network technologies operate, ensuring compatibility and reliability. The two most relevant standards for networking are:

IEEE 802.3: This standard defines Ethernet networks, which are wired connections. Ethernet is widely used for reliable, high-speed data transmission, especially in enterprise and data center environments, where performance and stability are critical.
IEEE 802.11: This standard governs Wireless Local Area Networks (WLANs), or what we commonly call Wi-Fi. Wireless networks are preferred for mobility and convenience, allowing devices to connect to the network without physical cables. This standard has evolved over the years with versions like 802.11a, 802.11n, 802.11ac, and more, each improving on speed, range, and bandwidth.
Wired (Ethernet) Networks
Ethernet networks provide a physical connection using cables like twisted pair, coaxial, or fiber optic. These connections are known for their high speed, low latency, and reliable performance. Ethernet is ideal for situations where you need stable, high-bandwidth connections, such as in:

Large office networks
Data centers
High-performance applications (e.g., video editing, gaming)
Ethernet cables connect devices like computers, servers, and switches directly to the network, which eliminates interference and offers a more secure connection. However, the main limitation is the lack of mobility—you are tied to where the cable reaches.

Wireless Networks (Wi-Fi)
Wireless networks, governed by IEEE 802.11, allow devices to connect to a network without using physical cables. This offers much greater mobility and flexibility, which is essential for modern devices like smartphones, laptops, and tablets. Wireless networks are ideal for home use, public spaces, and environments where mobility is a priority.

However, wireless networks have some limitations:

Interference: Wireless signals can be affected by interference from other electronic devices or physical obstacles like walls.
Security: Wireless networks are more susceptible to security breaches, so strong encryption (like WPA3) is necessary.
Range: Wireless connections typically have limited range and speed compared to wired connections, although advancements in standards like Wi-Fi 6 (802.11ax) have significantly improved performance.
Comparison

Wired: Provides faster, more reliable connections with lower latency, but requires physical cables, which limit mobility.
Wireless: Offers convenience and mobility, allowing devices to connect to the network without cables, but may experience interference, have limited range, and generally offers lower speeds compared to Ethernet.
In summary, wired networks (IEEE 802.3) offer high-speed, reliable connections, ideal for scenarios where performance is critical, while wireless networks (IEEE 802.11) offer flexibility and mobility, making them essential for everyday device connectivity. Both are standardized by the IEEE to ensure compatibility and performance across different devices and environments.


20 / 57
Goto...
P
Network Adapter
Converts instructions from upper layers into

Electrical signals
Optical signals
Wireless waves
Converts the received signals into meaningful data for the upper layers

Let’s talk about the network adapter, which is an essential component that enables devices to communicate over a network. Its main role is to translate data between the device and the network medium, whether that’s through wires, fiber optics, or wireless signals.

Converts Instructions from Upper Layers
The network adapter works by converting data from the higher layers of the network stack (like application, transport, and network layers) into signals that can travel over the physical network medium. This conversion can happen in three main forms:

Electrical signals: For wired connections like Ethernet, the adapter converts the data into electrical signals that travel through copper cables. Optical signals: For fiber optic connections, the adapter converts data into light pulses that are transmitted through fiber optic cables, allowing for extremely fast data transfer.
Wireless waves: For wireless connections (like Wi-Fi), the adapter converts data into radio waves that are transmitted through the air to communicate with a router or other wireless devices.
Converts Received Signals into Meaningful Data
In addition to sending data, the network adapter also receives signals from the network and converts them back into meaningful data that the higher layers of the network stack can understand. For example:

In a wired network, the adapter receives electrical signals and converts them into digital data that your computer or device can process.
In a wireless network, the adapter receives wireless signals and converts them into packets of data that are then passed to the upper layers for further processing and interpretation.
The network adapter acts as a translator between the physical network medium and the device, ensuring that data can be sent and received seamlessly across different types of networks.
In summary, the network adapter plays a critical role in converting data from the upper layers of the network stack into electrical, optical, or wireless signals for transmission. It also translates the received signals back into data that your device can process, making it essential for both wired and wireless network communication.


21 / 57
Goto...
P
Hub and Switch
Hub

You can find such devices in a museum
Simple signal repeater
Supports multiple ports
Star wiring and central point of wiring
Layer 1 device

Switch

Same functions as Hub + more
Intelligent signal repeater
Understand the source and destination address
Repeats signals only to proper destination ports
Layer 2 device
Some switches support Layer 3 and Layer 4 features
Let’s now compare two common network devices that are used to connect multiple devices within a network: Hubs and Switches. While both serve the purpose of connecting devices, they function at different layers of the network and have distinct capabilities.

1. Hub

A Device of the Past
Hubs are considered outdated and are rarely used in modern networks. You’d likely only find these in museums or very old network setups.
Simple Signal Repeater
A hub is a very basic network device. It acts as a signal repeater—it takes incoming data and repeats it out to all devices connected to its ports, regardless of the intended destination. This simplicity means that all devices connected to the hub receive the same data, whether it’s meant for them or not.

Supports Multiple Ports
Hubs typically have multiple ports to allow several devices to connect in a star topology, with the hub acting as the central point for all wiring.

Layer 1 Device
Hubs operate purely at Layer 1 of the OSI model, the Physical Layer, meaning they don’t understand anything about the data being transmitted, such as the source or destination. Their job is simply to repeat electrical signals.

In summary, hubs are simple and inefficient, as they broadcast data to all connected devices without any intelligence, leading to potential network congestion.

2. Switch

More Intelligent than a Hub
A switch performs the same basic function as a hub—it connects multiple devices in a network. However, it is far more intelligent in how it handles network traffic.
Intelligent Signal Repeater
Unlike a hub, a switch doesn’t just repeat signals blindly. It understands the source and destination addresses of data packets and only sends data to the correct destination port. This helps reduce unnecessary network traffic and improves efficiency.

Layer 2 Device
Switches operate at Layer 2 of the OSI model, the Data Link Layer. This allows them to use MAC addresses to identify devices on the network, making them more efficient at handling data traffic compared to hubs.

In summary, switches have largely replaced hubs because they can intelligently manage data flow within a network, ensuring that data is only sent to the devices that need it. This leads to improved performance and network efficiency. Today, switches are a standard part of any modern network infrastructure.


22 / 57
Goto...
P
VLAN
Network traffic isolation
Groups physical or virtual devices in a logical network
Flexible management
No need to rewire network
No need to move devices
Let’s dive into VLANs, or Virtual Local Area Networks, which provide a flexible and efficient way to manage network traffic and isolate different segments of the network.

Network Traffic Isolation
One of the primary purposes of a VLAN is to isolate network traffic. Even though devices might be physically connected to the same network infrastructure (such as the same switch), VLANs allow you to segment traffic logically. This means that data from one VLAN cannot directly communicate with another VLAN unless explicitly configured through a router or Layer 3 switch. This is crucial for improving security and performance by preventing unnecessary traffic from crossing into other segments of the network.

Groups Physical or Virtual Devices in a Logical Network
A VLAN can group physical or virtual devices into a logical network regardless of their physical location. For example, devices located on different floors of a building can be part of the same VLAN, allowing them to communicate as if they were on the same physical network. This logical grouping makes it easier to manage devices and traffic without being restricted by the physical layout of the network.

Flexible Management
VLANs provide a lot of flexibility in managing networks:

No Need to Rewire the Network: Since VLANs are configured in software, you don’t need to physically rewire the network when reorganizing devices or departments.
VLANs allow you to change the logical network topology without touching the physical infrastructure.
No Need to Move Devices: Similarly, you don’t have to physically move devices to group them into different network segments. With VLANs, you can reassign devices to different VLANs from your network management interface, saving time and effort.
In summary, VLANs offer a powerful way to isolate traffic and create logical networks within the same physical infrastructure. They give network administrators the flexibility to reorganize networks without the need for physical changes, making network management more efficient and scalable.


23 / 57
Goto...
P
Overlay Networks
Similar to VLAN but for big scale
A logical network built on top of a physical network
Cloud providers use overlay networks to isolate tenants
Overlay links are tunnels through the underlying physical network
Many overlay networks may coexist at once
Over the same underlying physical network
And providing its own isolation and services
Let’s discuss Overlay Networks, which are similar to VLANs but are designed for use at a larger scale, particularly in cloud environments and complex data centers.

Similar to VLAN but for Big Scale
Like VLANs, overlay networks provide logical separation of network traffic, but they are designed for large-scale environments. Overlay networks are commonly used in cloud infrastructures and multi-tenant environments where traffic isolation is needed at a much broader scale, often across data centers or regions.

A Logical Network Built on Top of a Physical Network
An overlay network is a logical network that is built on top of an existing physical network. The physical infrastructure (cables, switches, and routers) remains the same, but the overlay network creates additional layers of virtualized or logical networking on top of it. This abstraction enables more flexibility in how network traffic is managed and segmented.

Cloud Providers Use Overlay Networks to Isolate Tenants
In cloud environments, overlay networks are commonly used to isolate tenants. For example, a cloud provider like AWS, Azure, or Google Cloud might use overlay networks to ensure that the traffic from different customers (tenants) remains completely isolated, even though they are sharing the same underlying physical infrastructure. This ensures security, privacy, and performance for each tenant.

Overlay Links are Tunnels Through the Physical Network
The connections in an overlay network are often referred to as tunnels. These tunnels pass through the underlying physical network, but they are invisible to it. Instead, the physical network only sees regular traffic, while the overlay network manages its own logical links. Examples of technologies used for tunneling in overlay networks include VXLAN and GRE tunnels.

Many Overlay Networks Can Coexist
One of the key advantages of overlay networks is that multiple overlay networks can coexist at the same time, using the same underlying physical network. Each overlay network provides its own level of isolation and may offer specific services, such as security or traffic optimization, independently of other overlays. This flexibility is what makes overlay networks ideal for complex environments like cloud computing.

Over the same underlying physical network: Multiple overlay networks can run over the same physical infrastructure, without interfering with each other.
Providing its own isolation and services: Each overlay network can have its own rules, security policies, and traffic management, offering unique services while coexisting with other overlays.
In summary, Overlay Networks are logical networks that sit on top of physical networks, providing scalability, isolation, and flexibility in environments like cloud computing. They allow multiple tenants or services to operate independently, all while using the same physical infrastructure, making them crucial in modern large-scale network management.


24 / 57
Goto...
P
Router
Layer 3
Routes traffic between networks
Routes traffic based on routing table
Manages routing tables by routing protocols
Let’s talk about routers, which are essential devices in networking that help manage and direct traffic between different networks.

Layer 3 Device
A router operates at Layer 3 of the OSI model, also known as the Network Layer. This means it is responsible for managing how data is forwarded between different networks, unlike switches, which work primarily within a single network at Layer 2. Routers are vital for enabling communication between different networks, such as between a local network (LAN) and the internet.

Routes Traffic Between Networks
The primary function of a router is to route traffic between multiple networks. For example, if a device on your home network (LAN) wants to access a website on the internet, the router directs that traffic from your local network to the appropriate external network and vice versa. Routers ensure that data is sent efficiently across networks by selecting the best path for the data to travel.

Routes Traffic Based on Routing Table
Routers make decisions on where to send data based on a routing table. The routing table contains information about the possible routes to different network destinations, including the next hop and the distance to those destinations. When a router receives a data packet, it checks the destination IP address, looks it up in the routing table, and forwards the packet to the correct next hop or network. This process ensures that traffic is routed efficiently and reaches the right destination.

Manages Routing Tables Using Routing Protocols
Routers also manage routing tables by using routing protocols. These protocols automatically update the routing table based on changes in the network. These protocols allow routers to adapt to network changes and ensure that the routing table is always up to date with the best paths for traffic.

In summary, a router is a Layer 3 device that directs traffic between networks based on information in its routing table. It uses routing protocols to keep this table updated, ensuring efficient and accurate data routing across complex networks, making it a crucial element in both local and global networking.


25 / 57
Goto...
P
Routing Protocols
OSPF
BGP
IS-IS
IGRP
EIGRP
RIP
Now let’s look at some common routing protocols, which are essential for routers to dynamically determine the best path for data to travel across networks. Each protocol has its own method for calculating the most efficient route.

OSPF (Open Shortest Path First)
OSPF is a widely-used link-state routing protocol that calculates the shortest path to a destination based on various metrics, such as the cost of links. It is highly scalable and efficient for large enterprise networks. OSPF dynamically updates routers with the current network topology, ensuring that all routers have an identical view of the network and can make consistent routing decisions.

BGP (Border Gateway Protocol)
BGP is the primary protocol used to manage routing between large networks, like those that make up the internet. Unlike OSPF, BGP is a path-vector protocol, which means it doesn’t just look for the shortest path but instead considers various policies and preferences when choosing routes. BGP is crucial for maintaining internet connectivity and ensuring data can travel between autonomous systems (AS), which are large networks controlled by different organizations.

IS-IS (Intermediate System to Intermediate System)
IS-IS is another link-state protocol, similar to OSPF, but it is less common. Originally designed for the ISO protocol suite, IS-IS is used primarily in large service provider networks. It operates similarly to OSPF, calculating routes based on the shortest path and ensuring each router has a complete picture of the network.

IGRP (Interior Gateway Routing Protocol)
IGRP was developed by Cisco as a distance-vector protocol designed for routing within an autonomous system. It considers various metrics such as bandwidth, delay, and reliability to calculate the best route. However, IGRP has largely been replaced by its more advanced successor, EIGRP.

EIGRP (Enhanced Interior Gateway Routing Protocol)
EIGRP is an advanced version of IGRP and is also a Cisco proprietary protocol. It combines features of both distance-vector and link-state protocols, allowing for faster convergence and more efficient use of network resources. EIGRP dynamically adjusts routing decisions based on changes in the network, making it very efficient for large-scale networks.

RIP (Routing Information Protocol)
RIP is one of the oldest distance-vector protocols. It uses hop count as its only metric for determining the best path, with a maximum limit of 15 hops. While simple, RIP is not ideal for large or complex networks due to its limitations in scalability and slow convergence times. It’s mostly used in smaller or legacy networks.

In summary, routing protocols like OSPF, BGP, IS-IS, EIGRP, and RIP allow routers to dynamically update their routing tables and find the most efficient path for data to travel. Each protocol has its strengths and is used in different types of networks depending on factors such as size, complexity, and administrative control.


26 / 57
Goto...
P
Internet
The Internet is a system of interconnected networks that spans the globe


The Internet is the largest and most complex network in the world, often described as a global system of interconnected networks. Let’s break this down:

A System of Interconnected Networks
The Internet is not a single network but a massive collection of smaller networks—from local area networks (LANs) to wide area networks (WANs)—that are all interconnected. These networks belong to various organizations, governments, and service providers, yet they work together to allow seamless communication between devices and systems around the world.

Spans the Globe
The Internet has a truly global reach, connecting billions of devices from every corner of the planet. Whether you’re using a computer in an office, a smartphone on the go, or a server in a data center, the Internet allows you to access information and communicate with people from anywhere.

The vast infrastructure that makes up the Internet includes:

Undersea cables that connect continents
Satellites that link remote areas
Data centers that host critical services and store data, and Internet Service Providers (ISPs) that give users access to the Internet.
Collaboration of Autonomous Networks
The Internet works because it’s built on the collaboration of many autonomous networks. These networks communicate using standard protocols like TCP/IP, which ensure that data can travel from one point to another, regardless of the specific technologies or companies that own the networks in between.

In summary, the Internet is a vast, interconnected system of networks that enables global communication and access to information. It has transformed the way we live, work, and interact by allowing people and devices around the world to connect and share data.


27 / 57
Goto...
P
Intranets and Extranets
Intranets are:

A group of services hosted on a network
A private structure
Internet-like service provision
Extranets are:

Similar services to Intranet
Exposed to networks outside of the Intranet
Services that require extra security measures
Now let’s look at Intranets and Extranets, two important types of networks used by organizations to provide controlled access to information and services.

1. Intranets
An Intranet is essentially a private network that uses internet-like services but is restricted to an organization’s internal use. Let’s break it down:

A Group of Services Hosted on a Network
An intranet typically provides a wide range of services, such as file sharing, internal communication tools, collaboration platforms, and company information, all hosted within the organization’s own network infrastructure.

A Private Structure
The key characteristic of an intranet is that it’s a closed system—accessible only to authorized users within the organization. This ensures that sensitive information remains secure and protected from outside access.

Internet-Like Service Provision
While an intranet is private, it functions similarly to the public internet in terms of services offered. Employees can access websites, applications, and other resources using the same technologies (such as web browsers), but the resources are only available within the internal network.

2. Extranets
An Extranet is an extension of an intranet, allowing external parties to access certain services or information while maintaining strict security controls.

Similar Services to Intranet
Like an intranet, an extranet provides access to services such as collaboration tools, file sharing, and applications. However, it extends beyond the internal network to allow external users—such as business partners, vendors, or clients—to access specific resources.

Exposed to Networks Outside of the Intranet
An extranet is different from an intranet because it is exposed to users who are outside the organization’s internal network. For example, a company might allow a supplier to access inventory systems or a client to track project progress through an extranet.

Services That Require Extra Security Measures
Since an extranet provides access to users outside the organization, it requires extra security measures. This typically includes strong authentication, encryption, and firewalls to protect sensitive data and prevent unauthorized access.

In summary, Intranets are private networks that serve the internal needs of an organization, providing internet-like services to employees. Extranets, on the other hand, extend these services to authorized external users, but with additional security to protect the organization’s sensitive information.


28 / 57
Goto...
P
Firewall
A firewall is used to protect a private network from security risks inherent to connecting to an untrusted network
Rule based filtering
Anti-Virtus (AV) based filtering

Let’s talk about firewalls, which are essential for protecting private networks from potential security threats, especially when those networks are connected to the internet or other untrusted networks.

A Firewall Protects a Private Network from Security Risks
A firewall acts as a barrier between a trusted internal network (such as your company’s LAN) and an untrusted external network (such as the internet). Its primary purpose is to protect the private network from security risks by monitoring and controlling incoming and outgoing traffic based on predefined security rules. It essentially creates a filter that only allows safe and authorized traffic to pass through.

Rule-Based Filtering
Firewalls operate on a system of rule-based filtering. These rules define what kind of traffic is allowed or blocked. For example, you can create rules that:

Allow only certain IP addresses or ports to communicate with your network.
Block traffic from known malicious sources.
Only permit specific protocols such as HTTP or HTTPS.
By setting these rules, the firewall helps enforce security policies, preventing unauthorized access and stopping malicious data from entering the network.

Anti-Virus (AV) Based Filtering
Some advanced firewalls also include anti-virus (AV) filtering capabilities. These firewalls can scan incoming data (such as files or emails) for signs of malware or viruses before they are allowed into the network. If a threat is detected, the firewall can block the infected data and prevent it from spreading to internal devices. This adds an extra layer of protection against cyberattacks and helps secure your network against malicious software.

In summary, a firewall is a crucial component in protecting a private network from external threats. It does this by enforcing rule-based filtering to control traffic and, in some cases, using anti-virus filtering to scan for malware. This ensures that only safe and authorized data enters your network, keeping your systems secure.


29 / 57
Goto...
P
IDS and IPS
Intrusion Detection System (IDS)

Monitoring the events occurring in your network
Analyzing traffic for signatures that match known cyberattacks
Analyzing traffic for anomalies
Intrusion Prevention System (IPS)

the process of performing intrusion detection and then stopping the detected incidents
Let’s discuss Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS), two important technologies used to protect networks from cyberattacks by identifying and responding to suspicious activities.

1. Intrusion Detection System (IDS)
An Intrusion Detection System (IDS) is a monitoring system designed to track events occurring on your network and detect any unusual or malicious activities.

Monitoring the Events Occurring in Your Network
The main job of an IDS is to monitor network traffic and detect potential threats or suspicious activities. It acts as a watchdog, continuously observing the data flowing through the network.

Analyzing Traffic for Signatures that Match Known Cyberattacks
IDS works by analyzing traffic for signatures of known cyberattacks. Signatures are unique patterns or characteristics that match previously identified threats. For example, if a known malware tries to enter the network, the IDS will detect its signature and trigger an alert.

Analyzing Traffic for Anomalies
In addition to signature detection, IDS can also analyze traffic for anomalies—patterns that don’t match normal network behavior. Anomaly detection is crucial because it can help identify new or unknown threats that don’t have a signature yet. If the IDS detects unusual traffic or behavior, it raises an alert, allowing network administrators to investigate further.

2. Intrusion Prevention System (IPS)
An Intrusion Prevention System (IPS) builds on IDS technology but adds the ability to actively block or prevent detected threats. While IDS is focused on detection and alerting, IPS takes it a step further by automatically stopping the threat before it can cause damage. IPS can block malicious traffic in real time, helping to protect the network more proactively.

In summary:

IDS focuses on monitoring and detecting suspicious network activity by looking for known attack signatures and anomalies.
IPS not only detects threats but also actively prevents them by blocking malicious traffic in real time, offering an added layer of protection.
Together, IDS and IPS form a powerful defense against network intrusions and cyberattacks, helping to keep your network secure.

30 / 57
Goto...
P
VPN
Virtual Private Network
Method of extending a private network across public networks
Encryption is a common part of VPN
VPN Use Cases
Remote users
Mobile users
Site-to-Site
Let’s explore VPNs, or Virtual Private Networks, which are widely used to provide secure connections over public networks, such as the internet.

Virtual Private Network (VPN)
A VPN is a technology that allows you to extend a private network across a public network, like the internet. By doing so, it creates a secure connection that enables users to access private resources as if they were directly connected to the private network.

Method of Extending a Private Network Across Public Networks
A VPN essentially creates a virtual tunnel that securely passes data between a user’s device and the private network. Even though the data travels over a public network, it is protected and treated as if it were within a secure private network, ensuring that data confidentiality and integrity are maintained.

Encryption is a Common Part of VPN
Encryption is a key element of VPN technology. It ensures that all data passing through the VPN tunnel is encrypted, meaning that even if someone intercepts the traffic, they won’t be able to read or tamper with it. This provides an additional layer of security for users, especially when accessing sensitive data over public networks like Wi-Fi in coffee shops or airports.

VPN Use Cases

Remote Users: VPNs are commonly used by remote workers who need to access company resources from outside the office. By connecting through a VPN, they can access internal servers, files, and applications as if they were physically in the office.
Mobile Users: Similarly, mobile users (those working from various locations or on the go) use VPNs to securely connect to their organization’s network, ensuring their data is protected even when using untrusted public networks.
Site-to-Site: Site-to-site VPNs connect two or more separate networks, often between different branch offices or between a company’s office and a data center. This allows them to share resources securely over the internet, acting as if both locations are on the same private network.
In summary, a VPN is a powerful tool for securely extending a private network over public infrastructure. It uses encryption to protect data and offers versatile use cases such as enabling remote access for users or site-to-site connections between different locations.


31 / 57
Goto...
P
Proxy and Reverse Proxy Servers
Proxy servers

Cache information
Category based filtering
Anti-Virtus (AV) based filtering
Artificial Intelligent (AI) based filtering
Auditing and logging
Reverse proxy servers

Controlling web traffic
Securing web traffic
Offload HTTPS
Inspect traffic
Allow load balancing
Let’s discuss proxy servers and reverse proxy servers, two types of network intermediaries that are used to manage, filter, and secure network traffic in different ways.

1. Proxy Servers
A proxy server acts as an intermediary between a client and the internet. When a user requests a website or service, the request goes through the proxy server, which then forwards it to the destination. Proxy servers serve several functions:

Cache Information
Proxy servers can cache (store) copies of frequently accessed websites and resources. This reduces the time needed to retrieve information for users because cached content is delivered faster, and it reduces bandwidth consumption.

Category-Based Filtering
Many proxy servers are used for content filtering. For example, businesses or schools might block access to specific categories of websites (like social media, gambling, or adult content) by using category-based filtering. This helps enforce policies on what content can be accessed on the network.

Anti-Virus (AV) Based Filtering
Anti-virus filtering is another layer of protection that proxy servers can provide. They can scan incoming data for viruses and other malware before it reaches users, preventing malicious files or data from entering the network.

Artificial Intelligence (AI) Based Filtering Modern proxy servers can use AI-based filtering to recognize and block sophisticated threats or unwanted content. AI can detect patterns in network traffic that indicate unusual or malicious activity, providing a more proactive approach to security.

Auditing and Logging
Proxy servers are also useful for auditing and logging. They can keep track of users’ browsing activity, which helps in monitoring network usage, investigating security incidents, and ensuring compliance with company policies.

2. Reverse Proxy Servers
A reverse proxy server sits between web servers and the clients (such as web browsers) that are trying to access those servers. Its job is to manage incoming requests to the web server, providing several benefits:

Controlling Web Traffic
A reverse proxy manages incoming traffic to ensure that it is distributed efficiently to the appropriate servers. This is crucial for ensuring that web services remain available and perform well under heavy traffic.

Securing Web Traffic
Reverse proxy servers add an extra layer of security by shielding the backend servers from direct exposure to the public internet. They can block unwanted traffic, inspect incoming requests for threats, and protect sensitive systems behind them.

Offload HTTPS
Reverse proxy servers can offload HTTPS encryption by handling the SSL/TLS encryption and decryption processes, reducing the computational burden on the backend servers. This allows web servers to focus on processing the actual requests.

Inspect Traffic
Reverse proxies can also inspect incoming traffic for signs of attacks or malicious activity. This includes scanning for SQL injections, cross-site scripting (XSS), and other common web-based attacks.

Allow Load Balancing
One of the key functions of a reverse proxy is to act as a load balancer, distributing incoming traffic across multiple servers to ensure that no single server is overwhelmed. This improves performance and reliability, especially for websites or services with high traffic volumes.

In summary, proxy servers and reverse proxy servers both act as intermediaries but serve different purposes. Proxy servers focus on filtering, caching, and securing outgoing client requests, while reverse proxy servers manage and secure incoming traffic to web servers, improving security, performance, and load balancing.


32 / 57
Goto...
P
Load Balancer
What is a load balancer?

System that distributes incoming network traffic across multiple servers
Improves the availability, reliability, and performance
Types of load balancers

Layer 4 (Transport Layer)
Distributes traffic based on IP addresses and TCP/UDP ports
Faster but less flexible since it does not inspect the content of the data
Layer 7 (Application Layer)
Distributes traffic based on application-level data, such as HTTP headers, URLs, and cookies
Provides more control and flexibility for web applications
Let's talk about a critical component in modern infrastructure—load balancers. These systems ensure that no single server is overwhelmed with traffic, helping maintain the availability and reliability of your application.
What is a Load Balancer?

So, what exactly is a load balancer? In simple terms, a load balancer is a system that distributes incoming network traffic across multiple servers.
This helps spread the load evenly, ensuring that one server doesn’t get overloaded while others sit idle.
By distributing traffic in this way, load balancers improve the availability, reliability, and performance of your application. If one server fails, the load balancer will redirect traffic to healthy servers, minimizing downtime and service disruption.
Types of Load Balancers
Layer 4 (Transport Layer):

The first type is a Layer 4 load balancer, which operates at the Transport Layer of the OSI model.
It distributes traffic based on IP addresses and TCP/UDP ports.
This type of load balancer is fast because it doesn’t look at the content of the traffic. It just looks at the network and transport layer information to decide where to send the traffic.
However, since it doesn’t inspect the actual data, it is less flexible compared to Layer 7.
Layer 7 (Application Layer):

On the other hand, Layer 7 load balancers operate at the Application Layer.
They distribute traffic based on application-level data, such as HTTP headers, URLs, and cookies.
This gives you more control because you can route traffic based on content-specific rules. For instance, you can route traffic to different servers based on the URL path, which is very useful for web applications.
Although Layer 7 load balancers are a bit slower than Layer 4 because they inspect more data, they offer much more flexibility in terms of how you manage traffic.
Conclusion:

In summary, load balancers are crucial for distributing traffic and ensuring that your application remains available and responsive, even during periods of heavy load or when some servers are down.
Layer 4 is faster but simpler, while Layer 7 offers more control and flexibility, making it ideal for complex web applications.

33 / 57
Goto...
P
Load Balancer Key Features
Distribute Traffic
Balances incoming requests across multiple servers to prevent overloading any one server
Health Checks
Continuously monitors server health and removes unhealthy servers from the pool
Fault Tolerance
Ensures high availability by redirecting traffic away from failed or unresponsive servers
Scalability
Automatically routes traffic to additional servers when demand increases.

Now that we understand what a load balancer is and the different types, let’s dive into its core functions. These are the essential tasks that a load balancer performs to ensure your application runs smoothly and efficiently.
Distribute Traffic

First up, distributing traffic. The load balancer’s primary job is to balance incoming requests across multiple servers.
By doing this, it prevents any single server from becoming overloaded with too many requests. This distribution helps maintain server performance, ensuring that no one server slows down or crashes under heavy load.
So essentially, when multiple users access your application at the same time, the load balancer spreads that load out to keep everything running efficiently.
Health Checks

Another critical function is health checks. The load balancer continuously monitors the health of the servers in its pool.
If a server becomes unresponsive or has some kind of failure, the load balancer removes it from the pool, ensuring that no traffic is sent to that server.
This means your users will never be sent to a broken server, enhancing both reliability and user experience. Once the server recovers, it can be added back to the pool automatically.
Fault Tolerance

This brings us to fault tolerance. A load balancer ensures high availability by redirecting traffic away from failed or unresponsive servers.
If a server goes down, the load balancer automatically directs traffic to the remaining healthy servers, keeping your application available and minimizing downtime.
Fault tolerance is essential for maintaining service continuity, especially in mission-critical applications where downtime can have serious consequences.
Scalability

Lastly, let’s talk about scalability. As traffic to your application increases—let’s say during peak times or special events—the load balancer can automatically route traffic to additional servers."
This ensures your system can handle a larger load without slowing down or crashing.
This ability to scale on demand means that your application can grow smoothly, accommodating more users without requiring constant manual intervention. It’s especially important in cloud environments where resources can be scaled up or down as needed.
Conclusion:
So, by distributing traffic, performing health checks, ensuring fault tolerance, and scaling as demand grows, a load balancer plays a crucial role in keeping your application reliable, highly available, and efficient.


34 / 57
Goto...
P
Common Load Balancing Algorithms
Round Robin
Distributes requests to each server in sequential order
IP Hashing
Routes traffic based on the client’s IP address (requests from the same client go to the same server)
Least Connections
Sends traffic to the server with the fewest active connections
Sticky Sessions (Session Persistence)
Ensures that a client’s requests are always directed to the same server
Now that we’ve covered the basics of load balancers and their key functions, let’s talk about the different algorithms load balancers use to distribute traffic. Each algorithm follows a specific strategy to determine how incoming traffic is routed to servers.

Round Robin

Round Robin is the most straightforward algorithm. It distributes requests sequentially to each server in the pool.
Imagine a simple rotation: when a request comes in, it’s sent to Server A, the next one goes to Server B, then to Server C, and so on. Once all servers have been used, the cycle starts over.
This is great for environments where all servers have similar performance capabilities, as it evenly spreads the load across the servers.
IP Hashing

IP Hashing takes a different approach. It routes traffic based on the client’s IP address.
This means that a client’s requests will always be directed to the same server, ensuring consistency. For example, if I visit your website, my requests will consistently go to the same server as long as my IP address doesn’t change.
This is useful for applications where session persistence is needed but without relying on cookies or other forms of identification.
Least Connections

The Least Connections algorithm directs traffic to the server with the fewest active connections.
This is especially beneficial for systems with uneven workloads, where some requests take longer to process than others. By routing traffic to the least busy server, the load balancer optimizes performance and reduces response times.
For example, if Server A has 5 active connections, Server B has 3, and Server C has 7, the next request will go to Server B.
Sticky Sessions (Session Persistence)

Sticky Sessions, also known as Session Persistence, ensures that a client’s requests are always directed to the same server throughout their session.
This is important in applications where users maintain session data on the server—such as shopping carts in e-commerce sites or user profiles in web apps.
Sticky Sessions can be implemented using cookies or session IDs to ensure that the load balancer knows which server to route the client’s requests to.
Conclusion:

In summary, the choice of algorithm depends on the specific needs of your application. Round Robin is simple and effective for evenly distributed workloads, while IP Hashing and Sticky Sessions are essential when you need session persistence. Least Connections is ideal for applications with varying load times, ensuring that the server with the lightest load gets the next request.

35 / 57
Goto...
HTTP

36 / 57
Goto...
P
HTTP
Foundation of data communication on the World Wide Web
Enables the exchange of hypertext between clients and servers
Web Server and Browser
Follows a request-response model
Secure communications (using HTTPS)
Firewall friendly
Now, let’s discuss HTTP, which stands for Hypertext Transfer Protocol. It’s the fundamental protocol that drives data communication on the web, and almost everything we do on the internet relies on it in some form.
Foundation of Data Communication on the Web

HTTP is the foundation of data communication on the World Wide Web. Every time we browse a website, click a link, or submit a form, HTTP is working behind the scenes, allowing communication between our browser and the server.
Enables the Exchange of Hypertext

It’s specifically designed to enable the exchange of hypertext—documents that can link to other documents, such as web pages containing links, images, and multimedia content.
Web Server and Browser

HTTP connects two key components: the web browser on the client side and the web server on the server side. The browser sends HTTP requests to the server, and the server responds with the requested resources, such as HTML pages, images, or data.
Follows a Request-Response Model

One of the defining characteristics of HTTP is that it follows a request-response model. The client (the browser) sends a request, and the server processes that request and sends back a response. This model is stateless, meaning each request is independent of the others unless additional methods, like cookies or sessions, are used.
Secure Communications with HTTPS

We also have HTTPS, which is simply HTTP over SSL/TLS. This ensures that all communication between the client and server is encrypted, providing confidentiality, integrity, and authentication. HTTPS is crucial for securing sensitive data, like login credentials or payment information.
Firewall Friendly

HTTP is firewall-friendly. Since it operates over well-known ports like 80 for HTTP and 443 for HTTPS, most firewalls allow HTTP traffic by default. This makes it an easy choice for web communication without complicated firewall configurations.
Conclusion

HTTP is the backbone of communication on the web, allowing browsers and servers to exchange hypertext and other data in a simple, flexible, and—when needed—secure way through HTTPS.

37 / 57
Goto...
P
HTTP Key Features
Human-Readable
HTTP messages are typically in plain text
Request-Response Model
Client-server model where a client sends a request, and the server sends a response.
Stateless
Each request is independent
The server does not retain information about previous requests
Supports Caching
Allows responses to be cached to improve performance
Extensible
HTTP supports headers,
Headers allows the client and server to echange additional metadata
Uniform Resource Identification (URI)
Uses URIs to identify and locate resources
Different Data Formats
Allows for the transfer of various content types
Now that we’ve discussed HTTP as the foundation of web communication, let’s take a closer look at some of its key features that make it such a versatile and widely adopted protocol.
Human-Readable

One of the key advantages of HTTP is that it’s human-readable. The messages sent between the client and server are typically in plain text, making them easy to read and debug.
This simplicity helps developers understand and troubleshoot requests and responses without needing specialized tools.
Request-Response Model

HTTP follows a request-response model. This means the client—usually the browser—sends a request, and the server processes it and sends back a response.
This model is fundamental to how browsers interact with web servers, ensuring clear, structured communication.
Stateless

HTTP is stateless, meaning that each request is independent. The server does not retain any information about previous requests unless special mechanisms like cookies or sessions are used.
This design simplifies the protocol but requires additional methods for session management if we need to maintain user state across multiple requests.
Supports Caching

One of the performance-enhancing features of HTTP is its ability to support caching. Responses can be cached either on the client or on intermediary servers.
This reduces the need to repeatedly request the same resource, improving load times and decreasing bandwidth usage.
Extensible

HTTP is highly extensible, thanks to its support for headers. Both the client and server can use headers to exchange additional metadata beyond the core request or response.
For example, headers can be used to specify content types, define caching rules, or handle authentication. This flexibility makes HTTP adaptable to many different needs.
Uniform Resource Identification (URI)

HTTP uses URIs—Uniform Resource Identifiers—to identify and locate resources on the web. Each resource, such as a web page or an image, has its own unique URI, making it easy for clients to request specific content.
Different Data Formats

Finally, HTTP allows for the transfer of various data formats. It’s not limited to just HTML; it can handle JSON, XML, images, audio, video, and more.
This versatility makes HTTP suitable for a wide range of applications, from simple web pages to complex web services.
Conclusion

In summary, these features—human readability, statelessness, extensibility, and support for various data formats—make HTTP a powerful protocol that can handle different types of web communication efficiently.

38 / 57
Goto...
P
HTTP Versions
HTTP/1.0
Rarely used legacy version
opens a new connection for each request/response pair. And after each response the connection would be closed
HTTP/1.1
Widely used version
Supports persistent connections for multiple requests/responses over a single connection
HTTP/2
Introduced multiplexing
Allows multiple requests/responses to be handled in parallel over a single connection
Improving performance
HTTP/3
Built on QUIC (UDP based protocol), a transport layer protocol that reduces latency, improves performance, and enhances security
Now let’s take a closer look at the evolution of HTTP versions. Over the years, HTTP has been updated to improve performance, security, and reliability. We’ll go through the key differences between the major versions of HTTP.

HTTP/1.0

Starting with HTTP/1.0, which is a rarely used legacy version today.
In HTTP/1.0, a new connection was opened for each request and response. Once the response was sent, the connection was closed. This meant that every time a browser requested a resource, it had to open and close a new connection, which created significant overhead.
While it laid the foundation for the web, this approach quickly became inefficient, especially for web pages with multiple resources like images and scripts.
HTTP/1.1

Next, we have HTTP/1.1, which is still the most widely used version today.
The biggest improvement in HTTP/1.1 was the introduction of persistent connections. This means that after a connection is opened, multiple requests and responses can be sent over that single connection without needing to reopen and close it each time.
This feature drastically improved efficiency, as browsers could download multiple resources over the same connection, speeding up the loading of web pages.
HTTP/2

Then came HTTP/2, which introduced a major improvement called multiplexing.
With multiplexing, multiple requests and responses can be handled in parallel over a single connection. This means that the browser no longer has to wait for one request to finish before starting another.
This improvement significantly boosts performance by reducing the time it takes to load resources, especially for content-heavy websites.
HTTP/3

Finally, we have the most recent version, HTTP/3. HTTP/3 is built on QUIC, which is a UDP-based transport protocol.
This version is designed to reduce latency, improve performance, and enhance security.
By moving away from traditional TCP connections, HTTP/3 allows faster, more reliable connections even in environments with high packet loss or network congestion. * It’s a major step forward in making the web faster and more secure.
Conclusion In summary, HTTP has evolved significantly from HTTP/1.0, which was limited by connection overhead, to the modern HTTP/3, which is optimized for speed, security, and scalability. Each version builds on the previous one to meet the growing demands of web performance.


39 / 57
Goto...
P
HTTP Methods
GET
Retrieve data from the server (e.g., web pages, images)
HEAD
Ask for a response identical to a GET request, but without a response body
POST
Send data to the server (e.g., form submissions, file uploads)
PUT, DELETE, PATCH
Modify or delete resources on the server
HTTP provides a set of methods that define the different actions clients can request from servers. These methods are essential for interacting with web resources, and each serves a specific purpose. Let’s go through the most commonly used HTTP methods."
GET

The GET method is probably the most familiar. It’s used to retrieve data from the server, such as when you load a web page, request images, or download files.
GET requests are idempotent, meaning that making the same GET request multiple times will not change the state of the server. It simply returns data.
For example, when you type a URL into your browser, it sends a GET request to retrieve the web page.
HEAD

Next, we have the HEAD method, which is similar to GET, but with a key difference—it doesn’t return a response body.
This method asks for the headers only, allowing the client to check if the resource exists or get metadata without downloading the entire resource. It’s useful when you just want to know if a file is available or need to see information like content type or content length.
It’s a more efficient way to check for a resource without actually retrieving the data.
POST

The POST method is used to send data to the server, such as submitting a form on a website or uploading a file.
Unlike GET, POST requests can change the state of the server. For example, when you post a comment on a blog or submit a payment form, POST is being used to send that data to the server."
POST is typically used for creating new resources or submitting complex data, and it’s not idempotent, meaning multiple POST requests could result in creations of resources.
PUT, DELETE, PATCH

These methods are used for modifying or deleting resources on the server.
PUT: The PUT method is used to update an existing resource or create a new one if it doesn’t exist. Unlike POST, PUT is idempotent, meaning that sending the same request multiple times will result in the same outcome.
DELETE: As the name suggests, the DELETE method is used to remove a resource from the server. For example, deleting a user profile or removing a file.
PATCH: "Finally, PATCH is used to partially update a resource. While PUT replaces the entire resource, PATCH allows for more targeted updates, modifying only specific fields within the resource.
Conclusion

In summary, HTTP methods provide a standardized way to interact with resources on a server. GET retrieves data, POST sends data, and methods like PUT, DELETE, and PATCH are used to modify or delete resources.

40 / 57
Goto...
P
HTTP Status Codes
What are HTTP status codes?

Standardized response codes given by servers
Indicates the result of a client’s request
Common Status Code Categories

1xx: Informational
Indicates that the request has been received and is still being processed.
2xx: Success
Indicates that the request was successfully received, understood, and processed.
3xx: Redirection
Indicates that the client must take additional actions to complete the request.
4xx: Client Errors
Indicates that the client’s request contains bad syntax or cannot be fulfilled.
5xx: Server Errors
Indicates that the server failed to fulfill a valid request.
Introduction

When a client, such as a browser, makes a request to a server, the server responds with an HTTP status code. These are standardized codes that let the client know the result of their request.
Status codes provide essential information about whether the request was successful or if there were issues that need to be addressed.
What are HTTP Status Codes?

At their core, HTTP status codes are standardized response codes given by servers. They allow both the client and the user to understand the outcome of a request.
Each code is categorized into specific groups, based on the type of result, which makes troubleshooting and handling responses more efficient.
Common Status Code Categories
1xx: Informational

The 1xx status codes are informational. They indicate that the server has received the request and is still processing it. These are not very common in typical web browsing but are important in complex communication.
For example, 100 Continue tells the client that it can continue sending the request body.
2xx: Success

The 2xx range of codes means that the request was successfully received, understood, and processed.
The most common code is 200 OK, which indicates that the request was successfully handled and the server is returning the requested data.
Another example is 201 Created, which indicates that a new resource has been successfully created on the server, such as when a form is submitted.
3xx: Redirection

The 3xx codes indicate redirection. These tell the client that further action is needed to complete the request.
For example, 301 Moved Permanently indicates that the requested resource has been moved to a new URL, and all future requests should use the new location.
Another common one is 302 Found, which indicates that the resource is temporarily located at a different URL.
4xx: Client Errors

The 4xx codes represent client-side errors, meaning that there is something wrong with the request."
A common example is 404 Not Found, which means the server can’t find the requested resource, often because of a mistyped URL."
Another example is 400 Bad Request, which tells the client that the server couldn’t understand the request due to invalid syntax."
5xx: Server Errors

The 5xx codes represent server-side errors, meaning the server failed to fulfill a valid request."
For instance, 500 Internal Server Error is a generic error message indicating that something went wrong on the server’s side."
Another example is 503 Service Unavailable, which indicates that the server is temporarily unable to handle the request, often due to overload or maintenance."
Conclusion Understanding these categories helps in diagnosing issues with web requests. 1xx informs, 2xx means success, 3xx redirects, 4xx signals client errors, and 5xx points to server errors. Each category serves as a guide for what action, if any, needs to be taken next.


41 / 57
Goto...
P
HTTP Headers and Body
Headers:

Requests and responses contain headers
Metadata, such as content type or length
Body

Optional
The data being sent or received
Introduction
Let’s now talk about two important parts of an HTTP message: the headers and the body. These components allow clients and servers to communicate effectively by sending not just the main content, but also additional information.

Headers

Every HTTP request and response contains headers. These headers are pieces of metadata that provide important information about the request or response.
For example, headers can include the content type (like JSON or HTML), the length of the data being sent, or details about how the client should handle the response.
Headers act like instructions or descriptions, helping the server or client understand how to process the request or response correctly.
Body

The body is where the actual data being sent or received is placed. However, it’s optional—not all requests or responses will include a body.
For instance, a GET request typically doesn’t have a body because it’s simply asking for data, while a POST request will have a body that contains the form data or file uploads being sent to the server.
On the server’s side, a response body could contain the HTML of a web page, JSON data, or even binary files like images or videos.
Conclusion

In summary, headers provide essential metadata that helps guide the communication, while the body contains the actual content or data when needed. Together, they make up the structure of an HTTP message and ensure smooth data exchange.

42 / 57
Goto...
P
HTTP Cookies
What are cookies?

Small blocks of data created by a web server
Sent with each request to a server
Used to store stateful information (e.g. session identifier)
Stored as key-value pairs
Delivered using HTTP headers
Types of Cookies

Session Cookies
Persistent Cookies
Secure Cookies
HttpOnly Cookies
Let’s talk about HTTP Cookies, which play a critical role in maintaining state in web applications. While HTTP is a stateless protocol, cookies help keep track of user-specific data across different sessions or requests."

What are Cookies?

So, what exactly are cookies? They are small blocks of data created by a web server and sent to the client’s browser. Each time the client makes a request to the server, the cookie is automatically sent along with that request.
Cookies are commonly used to store stateful information—for instance, a session identifier that allows the server to recognize a returning user.
Cookies are stored as key-value pairs on the client side, and they are delivered using HTTP headers. This makes them a simple but powerful tool for maintaining state across what would otherwise be stateless interactions.
Types of Cookies:

Session Cookies

First, we have Session Cookies, which are temporary. They are stored in the browser’s memory and are deleted as soon as the browser is closed. These cookies are useful for short-term data storage, like when you log into a website but don’t want the session to persist beyond your browsing session."
Persistent Cookies

Next are Persistent Cookies. Unlike session cookies, these cookies are stored on the client’s device for a specified duration, which could be days, months, or even years. These are commonly used to remember login information, so users don’t have to log in every time they visit a website.
Secure Cookies

Then we have Secure Cookies, which are only sent over HTTPS connections. This ensures that sensitive information, like authentication tokens, is encrypted during transmission, protecting it from eavesdroppers."
HttpOnly Cookies

Lastly, HttpOnly Cookies cannot be accessed via JavaScript on the client side. This adds an extra layer of security, especially against cross-site scripting (XSS) attacks, where malicious scripts try to steal cookies.
Conclusion

In summary, cookies are a vital mechanism for maintaining stateful interactions in the inherently stateless HTTP protocol. They come in different forms, each suited to different use cases, from temporary session management to long-term user recognition and security.

43 / 57
Goto...
Modern Communication Protocols in Distributed Systems

44 / 57
Goto...
P
The era before the modern protocols
Inter-process communication (IPC)

Using pipes, shared memory, message queues, semaphores/mutex, COM, etc.
Inter-process communication over network

Using sockets, DCOM, RPC, CORBA, RMI, etc.
Before the rise of the modern protocols for distributed systems, applications used several methods for inter-process communication (IPC), both within a single system and across networks.

Inter-process Communication (IPC)
IPC refers to the methods used for communication between different processes running on the same machine. These methods include:

Pipes: A method for passing information from one process to another.
Shared Memory: A space in memory that multiple processes can access to communicate with each other.
Message Queues: A communication method where messages are stored in a queue and processed by receiving processes.
Semaphores/Mutex: Used for process synchronization, ensuring that multiple processes don’t access shared resources simultaneously in a conflicting way.
COM (Component Object Model): A Microsoft technology for enabling communication between different software components.
Inter-process Communication Over Networks
As networks became more common, there was a need for processes to communicate over the network. This led to the development of several network communication protocols:

Sockets: A low-level API for sending data across a network connection.
DCOM (Distributed Component Object Model): Microsoft’s extension of COM that allows communication between objects on different computers.
RPC (Remote Procedure Call): A protocol that allows a program to execute a procedure on a remote system as if it were a local call.
CORBA (Common Object Request Broker Architecture): A standard for enabling communication between objects in different systems, regardless of their programming language.
RMI (Remote Method Invocation): A Java API that allows objects in one Java virtual machine to invoke methods in objects running on another machine.

45 / 57
Goto...
P
Web Services
HTTP - the “de facto” web services transport protocol

Exchange objects and messages through HTTP protocol
Foundation of the World Wide Web
Universally accepted
Firewall friendly
Secure (HTTPS)
Variety of tools and languages
Let’s explore the concept of Web Services and the methods of communication that existed before web services became the standard for connecting systems and applications.

HTTP - The "De Facto" Web Services Transport Protocol
With the rise of the World Wide Web, HTTP became the “de facto” protocol for web services. HTTP’s flexibility and widespread use made it the perfect choice for exchanging data between systems over the internet.

Exchange Objects and Messages Through HTTP Protocol Web services use HTTP to exchange objects and messages between different systems. By using standard formats like XML or JSON, web services allow for easy communication between applications written in different languages.

Foundation of the World Wide Web HTTP is the foundation of the web, enabling browsers and servers to communicate. Web services build on this foundation to enable system-to-system communication over the same protocol.

Universally Accepted
HTTP is universally accepted, making it easy for web services to communicate across a variety of platforms and environments. This wide adoption means developers can use HTTP to build web services in virtually any programming language.

Firewall Friendly
Since HTTP is the primary protocol for web traffic, it is generally allowed through firewalls. This makes HTTP-based web services easier to deploy without running into network restrictions.

Secure (HTTPS)
Using HTTPS (the secure version of HTTP), web services can ensure that the data being transmitted is encrypted and secure, making it ideal for sensitive transactions.

Variety of Tools and Languages
Web services built on HTTP benefit from a wide range of tools and programming languages that support the protocol. This makes development and integration easier, with frameworks and libraries available for almost every major language.

In summary, before web services, inter-process communication was more complex and depended on specific methods and protocols. However, with the rise of HTTP as the standard web services transport protocol, systems can now communicate more easily and securely across platforms and networks, thanks to its universality, security, and tool support.


46 / 57
Goto...
P
SOAP and REST
SOAP

Simple Object Access Protocol
Strict rules to prepare request and response
Web Services Description Language (WSDL)
Relies heavily on XML
“Legacy”
REST

Representational State Transfer
Not very restrictive
Closer to Web in design philosophy
URI based
HTTP Verbs
HTTP Status codes
Variety of data formats (XML, JSON, YAML, etc)
Foundation for Microservices
Let’s compare two popular protocols used for web services: SOAP and REST. While both allow systems to communicate over the web, they have different philosophies, structures, and use cases.

1. SOAP (Simple Object Access Protocol)

Strict Rules to Prepare Requests and Responses
SOAP is a protocol with strict rules for formatting requests and responses. This makes it very rigid, but also ensures consistency and reliability, especially in complex enterprise environments where structure and error handling are critical.
Web Services Description Language (WSDL)
SOAP relies on WSDL (Web Services Description Language), an XML-based format that defines the interface for a web service. This includes the operations available and how the service can be called. WSDL acts as a contract between the service provider and the client.

Relies Heavily on XML
SOAP heavily relies on XML for message formatting. Both the request and response are wrapped in XML, making it highly structured but also relatively verbose, which can be a disadvantage when compared to lighter protocols like REST.

“Legacy” Although SOAP is still in use today, especially in large-scale enterprise environments and financial services, it is often considered “legacy” because it is being replaced by more flexible protocols like REST in modern applications.

REST (Representational State Transfer)
Not Very Restrictive
REST is much more flexible than SOAP. It does not have strict rules for how messages should be formatted, which allows developers to design APIs with greater freedom. This flexibility makes REST a better fit for modern web services.
Closer to Web in Design Philosophy REST aligns closely with the architecture of the web and how it is designed to work:

URI-based: REST uses URIs (Uniform Resource Identifiers) to identify resources. Each URI represents a specific resource that can be accessed or manipulated.
HTTP Verbs: REST makes use of HTTP methods such as GET, POST, PUT, DELETE, etc., to perform different operations on the resource.
HTTP Status Codes: REST also uses standard HTTP status codes (like 200 for success, 404 for not found, 500 for server errors) to provide feedback on the result of a request.
Variety of Data Formats: Unlike SOAP, which relies on XML, REST allows for a variety of data formats, including XML, JSON, YAML, and more. JSON has become the most popular format due to its simplicity and smaller payload size, making it ideal for web and mobile applications.
Foundation for many Distributed Services REST is widely used as the foundation for microservices architectures, where small, loosely coupled services communicate with each other over the web. Its flexibility, simplicity, and compatibility with modern web standards make it the preferred choice for scalable and distributed systems.

Comparison Summary

SOAP is more strict and structured, requiring XML and WSDL contracts, and is ideal for legacy systems or situations where formal agreements between services are necessary.
REST is more flexible and lightweight, uses HTTP methods, and supports a wide range of data formats, making it better suited for modern web applications and distributed architectures.
In conclusion, SOAP is a protocol that’s known for its strictness and structure, while REST is a more flexible and lightweight approach that fits better with the modern web and the rise of microservices.

47 / 57
Goto...
SOAP Example
HTTP/1.1 200 OK 
Date: Fri, 22 Nov 2013 21:09:44 GMT
Server: Apache/2.0.52 (Red Hat)
SOAPServer: SOAP::Lite/Perl/0.52
Content-Length: 566
Connection: close
Content-Type: text/xml; charset=utf-8
<?xml version="1.0" encoding="UTF-8"?>
<SOAP-ENV:Envelope xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xmlns:SOAP-ENC="http://schemas.xmlsoap.org/soap/encoding/"
xmlns:SOAP-ENV="http://schemas.xmlsoap.org/soap/envelope/"
xmlns:xsd="http://www.w3.org/2001/XMLSchema"
SOAP-ENV:encodingStyle="http://schemas.xmlsoap.org/soap/encoding/">
<SOAP-ENV:Body>
<namesp1:easter_dateResponse xmlns:namesp1="http://www.stgregorioschurchdc.org/Calendar"> 
	<s-gensym3 xsi:type="xsd:string">2014/04/20</s-gensym3>
</namesp1:easter_dateResponse>
     </SOAP-ENV:Body>
</SOAP-ENV:Envelope>


48 / 57
Goto...
REST Example
GET http://server/employees  HTTP/1.1
Host: server

HTTP/1.0 200 OK
Content-Type: application/json
Content-Length: 291
Access-Control-Allow-Origin: *
Server: Werkzeug/0.14.1 Python/3.6.7
Date: Thu, 28 Feb 2019 09:57:01 GMT
[
  {
    "id": 1,
    "name": "John Doe",
    "status": "enabled",
    "lastActionReason": "Hire new Python Developer."
  },
  {
    "id": 2,
    "name": "Doe Johnson",
    "status": "disabled",
    "lastActionReason": "Left the company."
  }
]


49 / 57
Goto...
P
REST and OpenAPI
What is OpenAPI?

Specification for defining RESTful APIs
Standardized and machine-readable format (JSON or YAML)
Originally known as Swagger
OpenAPI provides a structured way to document API endpoints
Request/Response formats
Expected behaviors
Allows code generation - Client and Server
Provides interactive documentation
Simplifies API lifecycle by making it easy to design, document, and share APIs
Main contributors - Microsoft, Google, Amazon, Red Hat, IBM and more.
Now, we’re going to look at OpenAPI, a specification that’s become essential for defining and documenting RESTful APIs. OpenAPI provides a consistent and structured way to describe API functionality, making it easier for developers to work with APIs at all stages of the development lifecycle.

What is OpenAPI?

OpenAPI is a specification for defining RESTful APIs in a way that’s both standardized and machine-readable. You’ll commonly see OpenAPI definitions written in JSON or YAML, making them easy to read for both humans and machines.
Some of you may know OpenAPI by its original name, Swagger. Initially developed as Swagger, it evolved into OpenAPI when it was contributed to the OpenAPI Initiative to become a collaborative, community-driven standard.
OpenAPI Provides a Structured Way to Document APIs

One of OpenAPI’s main strengths is that it provides a structured approach to documenting API endpoints. This means it covers all critical parts of an API, including request and response formats as well as expected behaviors for each endpoint.
With OpenAPI, developers can describe every endpoint in a standardized way, outlining the input parameters, response types, and status codes. This makes it easy for other developers to understand and interact with the API consistently.
Allows Code Generation - Client and Server

Another valuable feature of OpenAPI is its ability to generate code automatically. With OpenAPI definitions, you can generate both client libraries and server stubs. This means developers don’t have to start from scratch—they can use generated code as a foundation and then focus on business logic and customization.
This capability significantly reduces development time and helps ensure consistency across client and server implementations.
Provides Interactive Documentation

OpenAPI also enables interactive documentation, which is incredibly useful for developers. Tools like Swagger UI and Redoc can take an OpenAPI specification and turn it into interactive documentation where developers can not only view but also test API endpoints directly.
This interactive documentation helps developers understand the API more intuitively and makes it easier to troubleshoot or explore available endpoints.
Simplifies the API Lifecycle

Overall, OpenAPI simplifies the API lifecycle by making it easy to design, document, and share APIs. The structured format means that teams can easily communicate how the API is supposed to work, speeding up development and integration.
Main Contributors

OpenAPI has strong backing from some of the most influential tech companies, including Microsoft, Google, Amazon, Red Hat, IBM, and many others. These companies contribute to the ongoing development of the specification, ensuring that it remains relevant and aligned with industry needs.
Conclusion
In summary, OpenAPI is an essential tool for defining and documenting APIs. It provides a structured, standardized approach that improves consistency, supports code generation, enables interactive documentation, and simplifies the overall API lifecycle. OpenAPI has become a critical component of modern API development, thanks to contributions from leading tech companies.


50 / 57
Goto...
P
GraphQL
What is GraphQL?

Query language for APIs
Supports Query (Read) and Mutations (Create, Update, Delete)
Designed to provide flexible and efficient interaction with API
Developed by Facebook
Used by many popular apps like - Facebook, GitHub, Pinterest, Spotify and more.
GraphQL Principals

Get exactly what you need, nothing more and nothing less
Return predictable results
Get many resources in a single request
Evolve API without versions
Uses a schema that defines the structure of the data
All requests are sent to a single endpoint
Let’s dive into GraphQL, an advanced query language for APIs developed by Facebook. GraphQL is designed to provide a more flexible and efficient way to interact with APIs, addressing some of the limitations found in traditional REST-based approaches.

What is GraphQL?

GraphQL is a query language for APIs, allowing clients to request specific data in a structured way. It offers support for queries (to read data) and mutations (to create, update, or delete data).
Unlike traditional REST APIs, where multiple endpoints are often needed for different types of data, GraphQL enables all requests to be sent to a single endpoint.
It was initially developed by Facebook and has since been adopted by many major companies, including GitHub, Pinterest, and Spotify, who benefit from its flexibility and efficiency in handling complex data requirements.
GraphQL Principles

Get Exactly What You Need, Nothing More and Nothing Less:

One of the biggest advantages of GraphQL is that it allows clients to specify exactly the data they need. This means we avoid the issues of over-fetching (getting unnecessary data) and under-fetching (not getting enough data), which are common with REST APIs.
Return Predictable Results
GraphQL ensures predictable results by using a strongly-typed schema. This schema defines the structure and type of data, ensuring that both clients and servers understand the API format and reducing the chances of unexpected results.

Get Many Resources in a Single Request
With GraphQL, clients can retrieve multiple resources in a single request. This is extremely beneficial, especially in mobile and web applications, where reducing the number of requests improves performance and user experience.

Evolve API Without Versions
GraphQL makes it possible to evolve the API without creating new versions. This is done by extending the schema as needed, rather than creating separate API versions for each change, making it easier to manage and maintain.

Uses a Schema That Defines the Structure of the Data
As I mentioned, GraphQL uses a schema that acts as a contract between the client and the server. This schema defines what data is available, its type, and how it can be queried or mutated. This schema-driven approach makes the API self-documenting and easy to explore.

All Requests Are Sent to a Single Endpoint

Lastly, GraphQL simplifies API architecture by sending all requests to a single endpoint. This endpoint can handle complex queries and mutations, eliminating the need for multiple endpoints and simplifying the client-server interaction.
Conclusion
In summary, GraphQL provides a more efficient and flexible way to work with APIs. By allowing clients to request exactly what they need, supporting a single endpoint, and using a well-defined schema, GraphQL offers a powerful solution for modern application development.


51 / 57
Goto...
P
gRPC
What is gRPC?

Open-source high-performance communication protocol
Designed for building distributed systems and microservices
Initially created by Google
Uses HTTP/2 as the underlying transport protocol
Allows services to communicate efficiently using Protobuf (binary serialization data format)
gRPC Common Use Cases

Service to Service communication
Real-time Applications
Polyglot Environments
Next, let’s talk about gRPC, a powerful communication protocol widely used in modern distributed systems and microservices. gRPC was initially created by Google and has become the go-to choice for high-performance, real-time communication.

What is gRPC?

At its core, gRPC is an open-source, high-performance communication protocol that allows different services to communicate with each other efficiently.
It’s particularly designed for building distributed systems and microservices architectures, where multiple services need to talk to each other quickly and reliably.
gRPC uses HTTP/2 as the transport protocol, which enables bi-directional streaming, multiplexing, and better connection handling compared to older versions of HTTP.
For data exchange, gRPC uses Protobuf, a binary serialization format. This makes it faster and more efficient than using text-based formats like JSON or XML, as the data size is smaller, and it’s quicker to parse.
gRPC Common Use Cases:

Service to Service Communication:

One of the most common use cases for gRPC is service-to-service communication in microservices. In large systems, where services need to exchange data frequently and quickly, gRPC offers a performance boost due to its low latency and high throughput.
It’s particularly useful when you have numerous services that need to communicate across different environments or data centers.
Real-Time Applications:

gRPC shines in real-time applications as well. Thanks to its support for bi-directional streaming, gRPC allows both the client and server to send and receive data simultaneously, making it perfect for applications like live chat, video conferencing, or IoT systems, where data needs to be exchanged in real time.
Polyglot Environments:

Another major advantage of gRPC is its support for polyglot environments. Many organizations use different programming languages for different services, and gRPC allows you to easily integrate services written in multiple languages.
gRPC generates client and server code in a variety of languages, such as Go, Python, Java, C++, and more, allowing for seamless communication between services, regardless of the language they are written in.
Conclusion:
In summary, gRPC is a powerful and efficient protocol for high-performance communication between services. Whether you’re building microservices, real-time applications, or working in polyglot environments, gRPC provides the speed, efficiency, and flexibility needed to ensure smooth service-to-service communication.


52 / 57
Goto...
P
gRPC Key Features
Efficient Communication
Faster message sizes compared to traditional text-based formats like JSON or XML.
Bi-directional Streaming
Supports real-time communication with full-duplex streaming
Client and server can send data simultaneously
Strongly Typed Contracts
Define APIs using a .proto file
Ensures type safety and automatic code generation
Cross-Language Support
language-agnostic
Supports many languages such as C++, Go, Python, Java, Node.js, and more.
Let’s now explore the key features that make gRPC such a popular choice for building distributed systems and microservices. These features highlight how gRPC enhances communication efficiency, ensures strong typing, and supports a variety of languages.

Efficient Communication:

First, gRPC enables efficient communication by using Protobuf, a binary serialization format, which is much faster and more compact compared to traditional text-based formats like JSON or XML.
Because Protobuf messages are smaller and quicker to serialize and deserialize, gRPC reduces bandwidth usage and improves the performance of communication between services. This efficiency makes it a great option when high-performance communication is required.
Bi-directional Streaming:

One of gRPC’s standout features is its support for bi-directional streaming. This allows full-duplex communication, meaning that both the client and the server can send and receive data at the same time.
In traditional HTTP communication, the client sends a request, waits for the server to respond, and then continues. But with gRPC, the client and server can send multiple streams of data simultaneously, which is perfect for real-time applications like chat systems, live data feeds, or video streaming.
Strongly Typed Contracts:

gRPC also offers strongly typed contracts, which means that developers define the API using a .proto file. This file serves as a contract between the client and the server, ensuring that both sides adhere to the same structure and data types.
This provides type safety, meaning that mismatched data types are caught early, reducing bugs. Additionally, the .proto file enables automatic code generation, making it easy to create client and server code in multiple languages while maintaining consistency across services.
Cross-Language Support:

Lastly, gRPC is language-agnostic, which means it’s designed to work in cross-language environments. It supports many programming languages, including C++, Go, Python, Java, Node.js, and more.
This makes gRPC ideal for teams that are working in polyglot environments, where services are built in different languages. With gRPC, you can easily integrate these services without worrying about language compatibility, thanks to its strong support for multiple languages and auto-generated client-server code.
Conclusion:
In summary, gRPC’s key features—efficient communication, bi-directional streaming, strongly typed contracts, and cross-language support—make it a highly efficient and flexible protocol for building distributed systems that need to perform at scale.


53 / 57
Goto...
P
WebSockets
What are WebSockets?

Full-duplex communication channel over a single, long-lived connection
Allows real-time data exchange
Allows bi-directional communication without the need for multiple requests
WebSockets Common Use Cases

Real-Time Applications
Collaborative Tools
IoT (Internet of Things)
Now let’s talk about WebSockets, a protocol designed for real-time, bi-directional communication. WebSockets are widely used in modern web applications where real-time interaction is essential.

What are WebSockets?

At their core, WebSockets provide a full-duplex communication channel over a single, long-lived connection between the client and the server.
This means that once the connection is established, it remains open, allowing both the client and server to send and receive messages in real-time without the need to repeatedly open and close new connections.
Unlike traditional HTTP, where communication is request-response based, WebSockets enable bi-directional communication. The client and server can exchange data freely without waiting for the other to initiate communication, making WebSockets perfect for scenarios requiring instant updates.
WebSockets Common Use Cases:

Real-Time Applications:

One of the most common use cases for WebSockets is real-time applications. WebSockets allow instant, continuous data exchange, which is crucial in applications like live chat systems, gaming, and real-time financial updates.
For example, in a live chat application, WebSockets enable messages to be sent and received instantly without the delays that come with traditional HTTP.
Collaborative Tools:
WebSockets are also used in collaborative tools, where multiple users need to work together in real-time. Think about applications like Google Docs or collaborative whiteboards—WebSockets enable users to see updates and edits from other participants immediately, ensuring seamless collaboration.

IoT (Internet of Things):
Lastly, IoT (Internet of Things) applications often rely on WebSockets for real-time data streaming from devices. IoT devices like sensors, smart home devices, or even connected cars need to send and receive data continuously, and WebSockets provide the persistent connection necessary for that. By maintaining a long-lived connection, IoT devices can exchange data with servers and other devices instantly, allowing for real-time monitoring and control.

Conclusion: In summary, WebSockets offer a powerful solution for real-time, bi-directional communication, making them ideal for use cases like real-time applications, collaborative tools, and IoT systems. Their ability to maintain a persistent connection ensures that data can flow freely and instantly between clients and servers.


54 / 57
Goto...
P
WebSockets Key Features
Full-Duplex Communication:
Client and server can send and receive messages simultaneously over the same connection
Low Latency
No overhead from setting up and tearing down connections
Efficient Communication
After the initial connection, messages are exchanged without HTTP headers
Now that we understand what WebSockets are, let’s look at some of their key features that make them ideal for real-time communication. These features help explain why WebSockets are so effective for applications that require fast and continuous data exchange.

Full-Duplex Communication:

One of the standout features of WebSockets is that they provide full-duplex communication. This means that both the client and server can send and receive messages simultaneously over the same connection.
Unlike traditional HTTP, where the client sends a request and then waits for a response, WebSockets allow for two-way communication without having to wait. This makes it perfect for scenarios like live chats, real-time gaming, or collaborative tools, where data needs to flow in both directions instantly.
Low Latency:

Another key feature of WebSockets is their low latency. Since WebSockets establish a single, long-lived connection, there’s no overhead from continuously setting up and tearing down connections like in traditional HTTP.
This persistent connection reduces the time it takes to send and receive data, making WebSockets ideal for applications where speed is critical. For example, financial trading platforms or stock tickers benefit from this low-latency communication to provide real-time updates.
Efficient Communication:

WebSockets also enable efficient communication. Once the initial connection is established, messages can be exchanged without the need for HTTP headers in every message.
This reduces the size of each message, making data exchange faster and more efficient compared to the repeated overhead of HTTP requests. This efficiency is particularly useful in bandwidth-constrained environments or applications where a lot of data is being transmitted rapidly, such as IoT or multiplayer gaming.
Conclusion: In summary, the key features of WebSockets—full-duplex communication, low latency, and efficient message exchange—make them the go-to solution for real-time, performance-critical applications. These features allow WebSockets to provide fast, seamless communication between clients and servers, making them ideal for a wide range of modern applications.


55 / 57
Goto...
REST vs gRPC vs WebSockets
Feature	REST	GraphQL	gRPC	WebSockets
Communication Model	Request/Response	Request/Response	Request/Response & Streaming	Full-Duplex, Persistent
Transport Protocol	HTTP/1.1 or HTTP/2	HTTP/1.1 or HTTP/2	HTTP/2	TCP (Only the initial WebSocket handshake is HTTP)
Data Format	JSON, XML (text-based)	JSON	Protobuf (binary format)	Custom (binary or text-based)
Performance	Moderate (due to text-based payloads)	Moderate (Could be better than REST)	High (due to binary serialization)	High (low-latency, real-time communication)
Use Cases	Traditional web services, APIs	Fronted oriented APIs	Microservices, inter-service communication	Real-time applications (chat, gaming, IoT)
Streaming	No built-in support	No built-in support	Bi-directional streaming	Bi-directional streaming
Ease of Use	Widely adopted, simple to implement	Widely adopted, simple to implement	Requires .proto files, more setup overhead	Simple API, requires persistent connection
Language Support	Language-agnostic	Language-agnostic	Multi-language support	Language-agnostic
Session Management	Stateless	Stateless	Stateful	Stateful
Browser Support	All browsers	All browsers	No browser support	All modern browsers

56 / 57
Goto...
Demos and Labs
GitHub Repository
https://github.com/varadinov/devops_101_labs/tree/main/03

57 / 57
Goto...
